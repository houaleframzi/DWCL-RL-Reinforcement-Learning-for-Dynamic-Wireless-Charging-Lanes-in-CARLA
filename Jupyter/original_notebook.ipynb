{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.7.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#all imports\n",
    "import carla #the sim library itself\n",
    "import random #to pick random spawn point\n",
    "import cv2 #to work with images from cameras\n",
    "import numpy as np #in this example to change image representation - re-shaping\n",
    "import math\n",
    "import time\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"D:\\WindowsNoEditor\\PythonAPI\\carla\")  # Adjust this path\n",
    "import carla\n",
    "import pygame\n",
    "from agents.navigation.global_route_planner import GlobalRoutePlanner\n",
    "from agents.navigation.behavior_agent import BehaviorAgent\n",
    "from agents.navigation.local_planner import RoadOption\n",
    "from agents.navigation.local_planner import LocalPlanner\n",
    "from agents.navigation.basic_agent  import BasicAgent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CARLA environment\n",
    "# Constants\n",
    "eta_ch = 0.92  # Battery charging efficiency\n",
    "P_ev_max = 80  # Maximum Power Transfer (40 kW)\n",
    "Q1 = 90  # Quality factor of primary coil\n",
    "Q2 = 90 # Quality factor of secondary coil\n",
    "k0 = 0.2  # Nominal coupling coefficient (perfect alignment)\n",
    "d_max = 0.5  # Maximum lateral misalignment before no power transfer (meters)\n",
    "\n",
    "# Vehicle constants\n",
    "m = 1680  # kg, Mass\n",
    "g = 9.81  # m/s^2, Gravity\n",
    "c_rr = 0.01  # Rolling resistance coefficient\n",
    "c_d = 0.28  # Drag coefficient\n",
    "A = 1.93  # m^2, Frontal area\n",
    "rho = 1.20  # kg/m^3, Air density\n",
    "C_bat = 24  # kWh, Battery capacity\n",
    "alpha = 0.90  # Transmission efficiency\n",
    "beta = 0.75  # Regeneration efficiency\n",
    "P_aux = 0  # kW, Auxiliary power consumption\n",
    "max_speed = 70 / 3.6  # m/s\n",
    "avg_speed = 35 / 3.6\n",
    "avg_slop = 0.002\n",
    "avg_acc = 0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FakeWaypoint:\n",
    "    \"\"\"\n",
    "    A custom 'fake' waypoint that overrides:\n",
    "      - is_junction => always False\n",
    "      - lane_change => always Both\n",
    "    while forwarding all other attributes to the original waypoint.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, original_wp: carla.Waypoint):\n",
    "        self._wp = original_wp  # store the real waypoint internally\n",
    "\n",
    "    @property\n",
    "    def is_junction(self):\n",
    "        # ðŸš¦ Always return False to ignore intersections\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def lane_change(self):\n",
    "        # ðŸ”„ Always allow both left & right lane changes\n",
    "        return carla.LaneChange.Both\n",
    "\n",
    "    # Forward all other attributes / methods to the original waypoint\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._wp, name)\n",
    "\n",
    "\n",
    "class PatchedMap:\n",
    "    \"\"\"\n",
    "    Wraps the real carla.Map to ensure that every call to get_waypoint\n",
    "    or get_waypoint_xodr returns a FakeWaypoint instead of a real one.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, original_map: carla.Map):\n",
    "        self._original_map = original_map\n",
    "\n",
    "    def get_waypoint(self, location, project_to_road=True, lane_type=carla.LaneType.Driving):\n",
    "        wp = self._original_map.get_waypoint(location, project_to_road, lane_type)\n",
    "        if wp:\n",
    "            return FakeWaypoint(wp)  # wrap the real waypoint\n",
    "        return None\n",
    "\n",
    "    def get_waypoint_xodr(self, road_id, lane_id, s):\n",
    "        wp = self._original_map.get_waypoint_xodr(road_id, lane_id, s)\n",
    "        if wp:\n",
    "            return FakeWaypoint(wp)\n",
    "        return None\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # Forward all other map methods (like get_spawn_points, get_topology, etc.)\n",
    "        return getattr(self._original_map, name)\n",
    "\n",
    "\n",
    "def patch_carla_map(world: carla.World):\n",
    "    \"\"\"\n",
    "    Replaces world.get_map() with a lambda returning our PatchedMap,\n",
    "    so all code in all scripts see FakeWaypoint objects with:\n",
    "      - is_junction=False\n",
    "      - lane_change=Both\n",
    "    \"\"\"\n",
    "    original_map = world.get_map()\n",
    "    patched_map = PatchedMap(original_map)\n",
    "\n",
    "    def get_map_override():\n",
    "        return patched_map\n",
    "\n",
    "    # Monkey-patch the world object\n",
    "    world.get_map = get_map_override\n",
    "\n",
    "    print(\"âœ… Global patch applied: 'is_junction' = False, 'lane_change' = Both for all waypoints!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the sim \n",
    "client = carla.Client('localhost', 2000)\n",
    "# optional to load different towns\n",
    "client.set_timeout(15)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sim_env():\n",
    "\n",
    "    global carla_map \n",
    "    global all_corners\n",
    "    global length\n",
    "    global vehicle\n",
    "    global start_point\n",
    "    global new_start_point\n",
    "    global start_location\n",
    "    global world\n",
    "\n",
    "    world = client.load_world('Town06')\n",
    "\n",
    "    settings = world.get_settings()\n",
    "\n",
    "    # âœ… Disable rendering mode\n",
    "    settings.no_rendering_mode = True  # âœ… Turns off all 3D rendering\n",
    "    world.apply_settings(settings)  # âœ… Apply new settings\n",
    "\n",
    "    #define environment/world and get possible places to spawn a car\n",
    "   \n",
    "    \n",
    "    \n",
    "    #look for a blueprint of Mini car\n",
    "    vehicle_bp = world.get_blueprint_library().filter('nissan')\n",
    "    #spawn a car in a random location\n",
    "    \n",
    "    # Define the target location (x, y, z)\n",
    "    target_location = carla.Location(x=22.6, y=251, z=0.0)  # Modify with actual coordinates\n",
    "\n",
    "    # Get the map object\n",
    "    carla_map = world.get_map()\n",
    "\n",
    "    # Find the nearest waypoint to the target location\n",
    "    current_wp = carla_map.get_waypoint(target_location, project_to_road=True, lane_type=carla.LaneType.Driving)\n",
    "\n",
    "    start_point = current_wp.transform\n",
    "    \n",
    "    \n",
    "    new_start_point = current_wp.get_left_lane().transform\n",
    "    \n",
    "    vehicle = world.try_spawn_actor(vehicle_bp[0], new_start_point)\n",
    "\n",
    "    # move simulator view to the car\n",
    "    spectator = world.get_spectator()\n",
    "    start_point.location.z = start_point.location.z+1 #start_point was used to spawn the car but we move 1m up to avoid being on the floor\n",
    "    spectator.set_transform(start_point)\n",
    "\n",
    "    # Get the map object\n",
    "    carla_map = world.get_map()\n",
    "    all_corners = list()\n",
    "\n",
    "    # Get the waypoint for the starting position (assumes the car spawns in the leftmost lane)\n",
    "    start_waypoint = carla_map.get_waypoint(\n",
    "        start_point.location, \n",
    "        project_to_road=True, \n",
    "        lane_type=carla.LaneType.Driving\n",
    "    )\n",
    "\n",
    "    # Shift 4 lanes to the right\n",
    "    target_waypoint = start_waypoint\n",
    "    #for _ in range(4):\n",
    "     #   if target_waypoint.get_right_lane():\n",
    "      #      target_waypoint = target_waypoint.get_right_lane()\n",
    "       # else:\n",
    "        #    print(\"The road does not have 4 lanes to the right!\")\n",
    "         #   target_waypoint = None\n",
    "          #  break\n",
    "\n",
    "    # Ensure the target lane exists before proceeding\n",
    "    if target_waypoint:\n",
    "        # Define the starting location in the shifted lane\n",
    "        start_location = target_waypoint.transform.location\n",
    "\n",
    "        # Define the size of the \"plane\" (length, width)\n",
    "        length = 569  # Length of the \"plane\" (this determines how far ahead the lines go)\n",
    "        width = 3.5   # Width of the \"plane\" (lane width)\n",
    "\n",
    "        # Number of lines to draw, to cover most of the center of the lane\n",
    "        num_lines = 200  # You can increase this number for more lines\n",
    "\n",
    "        # Define the spacing between lines\n",
    "        line_spacing = width / num_lines\n",
    "\n",
    "        # Draw multiple lines in the center of the lane to simulate a \"plane\"\n",
    "        for i in range(num_lines):\n",
    "            # Calculate the y-coordinate for each line to simulate a series of parallel lines\n",
    "            y_offset = (i - num_lines / 2) * line_spacing  # Center the lines in the lane\n",
    "\n",
    "            # Define the start and end location of each line\n",
    "            start_line_location = carla.Location(\n",
    "                x=start_location.x, \n",
    "                y=start_location.y + y_offset, \n",
    "                z=start_location.z\n",
    "            )\n",
    "            \n",
    "            # End point 25 meters ahead\n",
    "            end_line_location = carla.Location(\n",
    "                x=start_line_location.x + length, \n",
    "                y=start_line_location.y, \n",
    "                z=start_line_location.z\n",
    "            )\n",
    "\n",
    "            # Define a color (e.g., Yellow)\n",
    "            color = carla.Color(r=0, g=1, b=0, a=0)\n",
    "\n",
    "            # Draw the line on the road\n",
    "            world.debug.draw_line(\n",
    "                start_line_location, \n",
    "                end_line_location, \n",
    "                thickness=0.05, \n",
    "                color=color, \n",
    "                life_time=0\n",
    "            )\n",
    "    else:\n",
    "        print(\"Could not draw the lines. Target lane does not exist!\")\n",
    "\n",
    "\n",
    "    # Ensure the target lane exists before proceeding\n",
    "    if target_waypoint:\n",
    "        start_location = target_waypoint.transform.location\n",
    "\n",
    "\n",
    "        # Define coil size\n",
    "        coil_width = 6 # Width of each coil (adjust as needed)\n",
    "        coil_height = 1  # Height (adjust as needed)\n",
    "\n",
    "        # Define coil positions based on the image (5m and 8m spacing, total 39m)\n",
    "        coil_positions = [ i for i in range(5,int(length),2+coil_width)]\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "        coil_color1 = carla.Color(r=1, g=1, b=1)\n",
    "        coil_color2 = carla.Color(r=1, g=1, b=0)\n",
    "        coil_color = coil_color2\n",
    "\n",
    "        # Draw each coil as a rectangle\n",
    "        for i,pos in enumerate(coil_positions):\n",
    "\n",
    "            if (i+1)%3==0 and coil_color == coil_color2:\n",
    "                coil_color = coil_color1\n",
    "            elif (i+1)%3==0 and coil_color == coil_color1:\n",
    "                coil_color = coil_color2\n",
    "\n",
    "\n",
    "            # Define coil color (Yellow)\n",
    "            \n",
    "            coil_center_x = start_location.x + pos  # Adjust X position\n",
    "            coil_center_y = start_location.y  # Keep Y position in lane center\n",
    "\n",
    "            # Define 4 corners of the rectangle\n",
    "            corners = [\n",
    "                carla.Location(x=coil_center_x - coil_width/2, y=coil_center_y - coil_height/2, z=start_location.z),\n",
    "                carla.Location(x=coil_center_x + coil_width/2, y=coil_center_y - coil_height/2, z=start_location.z),\n",
    "                carla.Location(x=coil_center_x + coil_width/2, y=coil_center_y + coil_height/2, z=start_location.z),\n",
    "                carla.Location(x=coil_center_x - coil_width/2, y=coil_center_y + coil_height/2, z=start_location.z)\n",
    "            ]\n",
    "            all_corners.append(corners)\n",
    "\n",
    "            # Draw the rectangle by connecting its corners\n",
    "            for i in range(4):\n",
    "                world.debug.draw_line(\n",
    "                    corners[i], \n",
    "                    corners[(i+1) % 4], \n",
    "                    thickness=0.1, \n",
    "                    color=coil_color, \n",
    "                    life_time=0\n",
    "                )\n",
    "    else:\n",
    "        print(\"Could not draw the coils. Target lane does not exist!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BatteryManager:\n",
    "    def __init__(self, vehicle, destination, world, power_model, C_bat, P_aux):\n",
    "        self.vehicle = vehicle\n",
    "        self.destination = destination  # Renamed from point_b\n",
    "        self.world = world\n",
    "        self.power_model = power_model\n",
    "        self.C_bat = C_bat\n",
    "        self.P_aux = P_aux\n",
    "        self.last_update_time = self.get_simulation_time()\n",
    "        self.last_vehicle_location = self.vehicle.get_location()\n",
    "        self.remaining_distance = self.compute_remaining_distance()\n",
    "        self.soc = 19  # State of Charge in percentage\n",
    "        self.total_energy = 0  # Total energy consumed (kWh)\n",
    "        self.prev_velocity = self.vehicle.get_velocity()  # Initialize in constructor\n",
    "        self.eta = None\n",
    "        self.soc_required = None\n",
    "        self.initial_eta = self.remaining_distance / avg_speed  \n",
    "        self.in_dwcl = self.is_on_charging_lane()\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_simulation_time(self):\n",
    "        return self.world.get_snapshot().timestamp.elapsed_seconds\n",
    "\n",
    "    def compute_remaining_distance(self):\n",
    "        carla_map = self.world.get_map()\n",
    "        grp = GlobalRoutePlanner(carla_map, 2.0)\n",
    "        route = grp.trace_route(self.vehicle.get_location(), self.destination)\n",
    "        return sum(route[i][0].transform.location.distance(route[i + 1][0].transform.location) for i in range(len(route) - 1))\n",
    "\n",
    "    def update_remaining_distance(self):\n",
    "        current_time = self.get_simulation_time()\n",
    "        vehicle_location = self.vehicle.get_location()\n",
    "        \n",
    "        self.remaining_distance = self.compute_remaining_distance()\n",
    "        self.last_update_time = current_time\n",
    "        self.last_vehicle_location = vehicle_location\n",
    "\n",
    "    def get_slope(self):\n",
    "        rotation = self.vehicle.get_transform().rotation\n",
    "        return math.radians(rotation.pitch)\n",
    "\n",
    "    def compute_forces(self, speed_m_s, acceleration):\n",
    "        slope = self.get_slope()\n",
    "        speed_vector = np.array([speed_m_s, avg_speed])\n",
    "        acc_vector = np.array([acceleration, avg_acc])\n",
    "        slope_vector = np.array([slope, avg_slop])\n",
    "        \n",
    "        F_roll = c_rr * m * g * speed_vector / 1000\n",
    "        F_climb = m * g * np.sin(slope_vector) * speed_vector / 1000\n",
    "        F_aero = 0.5 * c_d * A * rho * speed_vector**2 * speed_vector / 1000\n",
    "        F_acc = m * acc_vector * speed_vector  / 1000\n",
    "        \n",
    "        return F_roll, F_climb, F_aero, F_acc\n",
    "\n",
    "    def get_power_transfer_efficiency(self, k):\n",
    "        return (k**2 * Q1 * Q2) / (1 + k**2 * Q1 * Q2)\n",
    "\n",
    "    def get_coil_center_and_orientation(self, corners):\n",
    "        center_x = sum(c.x for c in corners) / 4\n",
    "        center_y = sum(c.y for c in corners) / 4\n",
    "        dx = corners[1].x - corners[0].x\n",
    "        dy = corners[1].y - corners[0].y\n",
    "        coil_length = math.sqrt(dx**2 + dy**2)\n",
    "        yaw_angle = math.degrees(math.atan2(dy, dx))\n",
    "        return carla.Location(center_x, center_y, corners[0].z), yaw_angle, coil_length\n",
    "\n",
    "    def is_on_charging_coil(self, all_corners):\n",
    "        epsilon = 0.25\n",
    "        vehicle_location = self.vehicle.get_location()\n",
    "        vehicle_x, vehicle_y = vehicle_location.x, vehicle_location.y\n",
    "        for corners in all_corners:\n",
    "            coil_center, coil_orientation, coil_length = self.get_coil_center_and_orientation(corners)\n",
    "            x_min, x_max = min(c.x for c in corners), max(c.x for c in corners)\n",
    "            y_min, y_max = min(c.y for c in corners), max(c.y for c in corners)\n",
    "            if x_min - epsilon <= vehicle_x <= x_max + epsilon and y_min - epsilon <= vehicle_y <= y_max + epsilon:\n",
    "                return True, coil_center, coil_orientation, coil_length\n",
    "        return False, None, None, None\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_plane_corners(self,length=569, width=3.5):\n",
    "        \"\"\"\n",
    "        Given the starting location and plane dimensions,\n",
    "        returns the 4 corner points of the plane.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute Y offsets\n",
    "        half_width = width / 2  # Half width to get the left and right bounds\n",
    "\n",
    "        # âœ… 4 Corner Points\n",
    "        top_left = carla.Location(\n",
    "            x=start_location.x, \n",
    "            y=start_location.y + half_width, \n",
    "            z=start_location.z\n",
    "        )\n",
    "\n",
    "        top_right = carla.Location(\n",
    "            x=start_location.x + length,  # Move `length` meters forward\n",
    "            y=start_location.y + half_width, \n",
    "            z=start_location.z\n",
    "        )\n",
    "\n",
    "        bottom_left = carla.Location(\n",
    "            x=start_location.x, \n",
    "            y=start_location.y - half_width, \n",
    "            z=start_location.z\n",
    "        )\n",
    "\n",
    "        bottom_right = carla.Location(\n",
    "            x=start_location.x + length,  # Move `length` meters forward\n",
    "            y=start_location.y - half_width, \n",
    "            z=start_location.z\n",
    "        )\n",
    "\n",
    "        return top_left, top_right, bottom_left, bottom_right\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def is_on_charging_lane(self):\n",
    "        \"\"\"\n",
    "        Checks if the vehicle is inside the rectangular plane formed by the given corners.\n",
    "        \n",
    "        :param vehicle: The CARLA vehicle actor.\n",
    "        :param plane_corners: A tuple of (top_left, top_right, bottom_left, bottom_right) carla.Location\n",
    "        :return: True if the vehicle is inside the plane, False otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        # ðŸš— Get vehicle position\n",
    "        vehicle_location = self.vehicle.get_location()\n",
    "        vehicle_x, vehicle_y = vehicle_location.x, vehicle_location.y\n",
    "\n",
    "       \n",
    "\n",
    "        # ðŸŽ¯ Unpack the plane's 4 corners\n",
    "        top_left, top_right, bottom_left, bottom_right = self.get_plane_corners()\n",
    "\n",
    "        # ðŸ“Œ Define the rectangular bounds\n",
    "        min_x = min(top_left.x, top_right.x, bottom_left.x, bottom_right.x)  # Smallest x\n",
    "        max_x = max(top_left.x, top_right.x, bottom_left.x, bottom_right.x)  # Largest x\n",
    "        min_y = min(top_left.y, top_right.y, bottom_left.y, bottom_right.y)  # Smallest y\n",
    "        max_y = max(top_left.y, top_right.y, bottom_left.y, bottom_right.y)  # Largest y\n",
    "\n",
    "        # âœ… Check if the vehicle is inside the bounds\n",
    "        inside_x = min_x <= vehicle_x <= max_x\n",
    "        inside_y = min_y <= vehicle_y <= max_y\n",
    "\n",
    "        return inside_x and inside_y  # True if inside both x & y bounds\n",
    "\n",
    "\n",
    "    def get_alignment_factor(self, coil_center, coil_orientation, coil_length):\n",
    "        l_max = coil_length\n",
    "        vehicle_location = self.vehicle.get_location()\n",
    "        vehicle_x, vehicle_y = vehicle_location.x, vehicle_location.y\n",
    "        d_y = abs(vehicle_y - coil_center.y)\n",
    "        f_lat = np.exp(- (d_y / d_max) ** 2)\n",
    "        d_x = abs(vehicle_x - coil_center.x)\n",
    "        f_long = np.exp(- (d_x / l_max) ** 2)\n",
    "        vehicle_rotation = self.vehicle.get_transform().rotation.yaw\n",
    "        theta_misalign = abs(vehicle_rotation - coil_orientation)\n",
    "        theta_misalign_rad = math.radians(theta_misalign)\n",
    "        f_ang = abs(np.cos(theta_misalign_rad) + 0.5 * np.sin(theta_misalign_rad))\n",
    "        f_align = f_lat * f_long * f_ang\n",
    "        k = k0 * f_align\n",
    "        return f_align, k\n",
    "    \n",
    "\n",
    "    def get_acceleration(self,delta_time):\n",
    "       \n",
    "        \n",
    "        if delta_time <= 0:\n",
    "            return 0  # Avoid division by zero or negative time steps\n",
    "\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed_m_s = math.sqrt(velocity.x**2 + velocity.y**2)\n",
    "\n",
    "        if hasattr(self, \"prev_velocity\") and self.prev_velocity is not None:\n",
    "            prev_speed_m_s = math.sqrt(self.prev_velocity.x**2 + self.prev_velocity.y**2)\n",
    "            acceleration = (speed_m_s - prev_speed_m_s) / delta_time\n",
    "        else:\n",
    "            acceleration = 0  # First time, assume no acceleration\n",
    "\n",
    "        # Update for next calculation\n",
    "        self.prev_velocity = velocity\n",
    "        \n",
    "        \n",
    "\n",
    "        return acceleration\n",
    "    \n",
    "\n",
    "\n",
    "    def get_target_dwcl_lane_id(self):\n",
    "        \"\"\"ðŸ” Find the lane ID of the nearest DWCL.\"\"\"\n",
    "        min_distance = float('inf')\n",
    "        target_lane_id = None\n",
    "\n",
    "        for corners in all_corners:\n",
    "            dwcl_wp = self.vehicle.get_world().get_map().get_waypoint(corners[0])\n",
    "            distance = self.vehicle.get_location().distance(dwcl_wp.transform.location)\n",
    "\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                target_lane_id = dwcl_wp.lane_id\n",
    "\n",
    "        return target_lane_id  # Returns the lane ID of the closest DWCL\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def update_battery(self, delta_time, all_corners):\n",
    "        location = self.vehicle.get_location()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed_m_s = np.sqrt(velocity.x**2 + velocity.y**2)\n",
    "        acceleration = self.get_acceleration(delta_time)\n",
    "        slope = self.get_slope() * 57.2958\n",
    "        F_roll, F_climb, F_aero, F_acc = self.compute_forces(speed_m_s, acceleration)\n",
    "        df = pd.DataFrame({\n",
    "            'aeroF_P': F_aero,\n",
    "            'climbF_P': F_climb,\n",
    "            'rollF_P': F_roll,\n",
    "            'accF_P': F_acc\n",
    "        })\n",
    "        P_net, req_P = -power_model.predict(df, verbose=0).flatten() #in kW\n",
    "        P_net = P_net  * 0 if speed_m_s <=0 else P_net\n",
    "        P_total = delta_time * (P_aux + P_net)  # Convert to kWh\n",
    "        T_req_P = (P_aux + req_P)\n",
    "\n",
    "        # Compute remaining distance\n",
    "        #remaining_distance = abs(point_b.x - location.x)\n",
    "        self.update_remaining_distance()\n",
    "        remaining_distance = self.compute_remaining_distance()\n",
    "\n",
    "        # Compute energy consumption\n",
    "        energy_consumed = (1 / 3600) * P_total  # Convert W to kWh\n",
    "        energy_required = (remaining_distance / avg_speed) / 3600 * T_req_P\n",
    "        soc_required = (energy_required / C_bat) * 100\n",
    "        on_charging_lane, coil_center, coil_orientation, coil_length = self.is_on_charging_coil(all_corners)\n",
    "        if on_charging_lane:\n",
    "            f_align, k = self.get_alignment_factor(coil_center, coil_orientation, coil_length)\n",
    "            eta_transfer = self.get_power_transfer_efficiency(k)\n",
    "            P_ev = P_ev_max * eta_transfer * eta_ch\n",
    "            energy_charged = (P_ev * delta_time) / 3600\n",
    "        else:\n",
    "            energy_charged, P_ev, f_align, k, eta_transfer = 0, 0, 0, 0, 0\n",
    "        self.total_energy += (energy_consumed - energy_charged)\n",
    "        self.soc -= ((energy_consumed - energy_charged) / C_bat) * 100\n",
    "        self.soc = min(max(self.soc, 0), 100)\n",
    "        self.total_energy = max(0,self.total_energy)\n",
    "        \n",
    "        self.eta = remaining_distance / (avg_speed + 0.01)\n",
    "        self.soc_required = soc_required\n",
    "        self.remaining_distance = remaining_distance\n",
    "        #print('eta inside battery_manager:',self.eta)\n",
    "        \n",
    "        return self.soc, soc_required, self.total_energy, self.eta, P_ev, f_align, k, eta_transfer, remaining_distance,acceleration,slope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import collections\n",
    "import carla\n",
    "import time\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "global lane_switch_counter\n",
    "lane_switch_counter = 0\n",
    "global invalid_action_counter\n",
    "invalid_action_counter = 0\n",
    "\n",
    "\n",
    "\n",
    "class CustomBehaviorAgent(BehaviorAgent):\n",
    "    \"\"\"ðŸš— Custom Behavior Agent that ignores red lights.\"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def run_step(self, debug=False):\n",
    "        \"\"\"ðŸš€ Runs a step but prevents crashes when waypoints are missing.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # âœ… Try running the normal Behavior Agent step\n",
    "            return super().run_step(debug)\n",
    "        \n",
    "        except AttributeError as e:\n",
    "            #print(f\"ðŸš¨ Warning: Missing waypoint! Error: {e}\")\n",
    "            return carla.VehicleControl()  # Return empty control to avoid crashing\n",
    "\n",
    "\n",
    "\n",
    "    def _update_information(self):\n",
    "        \"\"\"Overrides the default function to skip red light checks.\"\"\"\n",
    "        super()._update_information()\n",
    "\n",
    "        # âœ… Ignore Traffic Light States (Override the check)\n",
    "        if self._vehicle.get_traffic_light_state() == carla.TrafficLightState.Red:\n",
    "            #print(\"ðŸš¦ Ignoring red light...\")\n",
    "            self._ignore_traffic_light = True  # Skip red light checks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ðŸš— CARLA ENVIRONMENT\n",
    "class CarlaEnv(gym.Env):\n",
    "    def __init__(self, vehicle, battery_manager, all_corners, destination):\n",
    "        super(CarlaEnv, self).__init__()\n",
    "        \n",
    "        self.vehicle = vehicle\n",
    "        self.battery_manager = battery_manager\n",
    "        self.all_corners = all_corners\n",
    "        self.destination = destination\n",
    "        self.start_time = self.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds\n",
    "        #self.global_planner,self.local_planner = self.setup_route_planner()\n",
    "        self.tm = self.set_up_traffic_manager()\n",
    "        #self.agent = self.initialize_behavior_agent()\n",
    "        self.target_speed = 40  # Set speed to 40 km/h\n",
    "\n",
    "        # ðŸ”¹ State Space: [SoC, Required SoC, ETA, Distance to DWCL End, Lane Type, Target_speed]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0, 0, 5]),\n",
    "            high=np.array([100, 100, 1000, 1000, 1, 100]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # ðŸ”¹ Action Space: [Go to DWCL, Leave DWCL, Accelerate, Decelerate, Maintain Speed, Stay Out DWCL]\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "\n",
    "    def get_current_lane_id(self):\n",
    "        \"\"\"ðŸ” Retrieves the current lane ID of the vehicle.\"\"\"\n",
    "        map = self.vehicle.get_world().get_map()\n",
    "        waypoint = map.get_waypoint(self.vehicle.get_location())\n",
    "        return waypoint.lane_id\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def initialize_behavior_agent(self):\n",
    "        \"\"\"\n",
    "        Initializes a Behavior Agent for autonomous navigation to a given destination.\n",
    "\n",
    "        Args:\n",
    "            vehicle (carla.Vehicle): The vehicle that will be controlled by the agent.\n",
    "            destination_location (carla.Location): The target destination.\n",
    "\n",
    "        Returns:\n",
    "            BehaviorAgent: The initialized agent ready to navigate.\n",
    "        \"\"\"\n",
    "\n",
    "        # âœ… Step 1: Create the Behavior Agent\n",
    "        agent = CustomBehaviorAgent(self.vehicle, behavior=\"normal\",opt_dict={'ignore_traffic_lights': True})  # Options: \"normal\", \"aggressive\", \"cautious\"\n",
    "\n",
    "        # âœ… Step 2: Set the destination\n",
    "        agent.set_destination(self.destination)\n",
    "\n",
    "        agent._local_planner._min_waypoint_queue_length = 10\n",
    "        agent._local_planner._buffer_size = 20\n",
    "        agent.target_speed = 40  # Set speed to 40 km/h\n",
    "\n",
    "        # âœ… Disable Traffic Manager interference\n",
    "        client = carla.Client(\"localhost\", 2000)  # Ensure correct connection\n",
    "        traffic_manager = client.get_trafficmanager(8000)\n",
    "        \n",
    "        traffic_manager.auto_lane_change(vehicle, False)\n",
    "        agent._steering_gain = 0.5\n",
    "\n",
    "        #print(f\"âœ… Behavior Agent initialized. Navigating to: {self.destination}\")\n",
    "\n",
    "        # âœ… Step 3: Return the agent for further use\n",
    "        return agent\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def get_nearest_dwcl_location(self):\n",
    "        \"\"\"ðŸ” Find the lane ID of the nearest DWCL.\"\"\"\n",
    "        min_distance = float('inf')\n",
    "        target_lane_id = None\n",
    "\n",
    "        for corners in self.all_corners:\n",
    "            dwcl_wp = self.vehicle.get_world().get_map().get_waypoint(corners[0])\n",
    "            distance = self.vehicle.get_location().distance(dwcl_wp.transform.location)\n",
    "\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                target_lane_id = dwcl_wp.lane_id  # âœ… Store correct DWCL lane\n",
    "                target_lane_location = dwcl_wp.transform.location\n",
    "                target_lane_transform = dwcl_wp.transform\n",
    "                velocity = vehicle.get_velocity()\n",
    "\n",
    "                # Speed in m/s\n",
    "                speed_m_s = math.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "\n",
    "                # Speed in km/h\n",
    "                speed_kmh = speed_m_s * 3.6\n",
    "                brake_safe_dist = (speed_kmh/8)**2\n",
    "                if speed_kmh < 15 and brake_safe_dist < 30:\n",
    "                    brake_safe_dist = 30\n",
    "                target_lane_transform.location.x += brake_safe_dist  \n",
    "                #target_lane_location.y+=10\n",
    "                \n",
    "\n",
    "        # âœ… Get a pedestrian blueprint\n",
    "        blueprint_library = world.get_blueprint_library()\n",
    "        pedestrian_bp = random.choice(blueprint_library.filter('walker.pedestrian.*'))\n",
    "\n",
    "        # âœ… Define spawn location\n",
    "        #pedestrian_spawn = carla.Transform(carla.Location(x=50, y=-10, z=1))\n",
    "\n",
    "        # âœ… Spawn pedestrian\n",
    "        #pedestrian = world.try_spawn_actor(pedestrian_bp, carla.Transform(target_lane_location))\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        #print(f\"ðŸ“Œ Target DWCL Lane Location: {target_lane_location}\")  # âœ… Debugging output\n",
    "        return target_lane_transform  # âœ… Returns correct DWCL lane ID\n",
    "    \n",
    "\n",
    "\n",
    "    def get_nearest_NOdwcl_location(self):\n",
    "        \"\"\"ðŸ” Find the lane ID of the nearest DWCL.\"\"\"\n",
    "        min_distance = float('inf')\n",
    "        target_lane_id = None\n",
    "        # Get the current vehicle location\n",
    "        vehicle_location = self.vehicle.get_location()\n",
    "\n",
    "        # Get the map object from the world\n",
    "        carla_map = self.vehicle.get_world().get_map()\n",
    "\n",
    "        # Get the closest waypoint to the vehicle location\n",
    "        current_wp = carla_map.get_waypoint(vehicle_location, project_to_road=True, lane_type=carla.LaneType.Driving)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        left_wp = current_wp.get_left_lane() \n",
    "       \n",
    "        target_lane_transform = left_wp.transform\n",
    "        velocity = vehicle.get_velocity()\n",
    "\n",
    "        # Speed in m/s\n",
    "        speed_m_s = math.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "\n",
    "        # Speed in km/h\n",
    "        speed_kmh = speed_m_s * 3.6\n",
    "        brake_safe_dist = (speed_kmh/8)**2\n",
    "        if speed_kmh < 15 and brake_safe_dist < 20:\n",
    "            brake_safe_dist = 20\n",
    "        target_lane_transform.location.x += brake_safe_dist  \n",
    "        #target_lane_location.y+=10\n",
    "                \n",
    "\n",
    "        # âœ… Get a pedestrian blueprint\n",
    "        blueprint_library = world.get_blueprint_library()\n",
    "        pedestrian_bp = random.choice(blueprint_library.filter('walker.pedestrian.*'))\n",
    "\n",
    "        # âœ… Define spawn location\n",
    "        #pedestrian_spawn = carla.Transform(carla.Location(x=50, y=-10, z=1))\n",
    "\n",
    "        # âœ… Spawn pedestrian\n",
    "        #pedestrian = world.try_spawn_actor(pedestrian_bp, carla.Transform(target_lane_location))\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        #print(f\"ðŸ“Œ Target DWCL Lane Location: {target_lane_location}\")  # âœ… Debugging output\n",
    "        return target_lane_transform  # âœ… Returns correct DWCL lane ID\n",
    "    \n",
    "    def stop_car(self):\n",
    "\n",
    "        self.vehicle.set_autopilot(False)\n",
    "        control = carla.VehicleControl()\n",
    "        control.throttle = 0.0\n",
    "        control.brake = 1.0  # Full brake\n",
    "        control.hand_brake = False\n",
    "        control.steer = 0.0\n",
    "        \n",
    "\n",
    "        # âœ… Apply stopping control\n",
    "        self.vehicle.apply_control(control)\n",
    "        #for _ in range(26):\n",
    "        #     self.vehicle.get_world().tick()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def setup_route_planner(self):\n",
    "        \"\"\"ðŸ—ºï¸ Initializes the CARLA Global Route Planner\"\"\"\n",
    "        world_map = self.vehicle.get_world().get_map()\n",
    "        #print(\"ðŸ—ºï¸ Route Planner Initialized\")\n",
    "        return GlobalRoutePlanner(world_map, 0.5), LocalPlanner(self.vehicle, opt_dict={\"target_speed\": 30})  # 2.0 meters precision\n",
    "    \n",
    "    def set_up_traffic_manager(self,reduce_speed_by=0):\n",
    "        \"\"\"ðŸš¦ Configures CARLA's Traffic Manager to ensure smooth and safe driving to the DWCL.\"\"\"\n",
    "        client = carla.Client(\"localhost\", 2000)  # Ensure correct connection\n",
    "        self.tm = client.get_trafficmanager(8000)  # âœ… Use correct API\n",
    "\n",
    "        # âœ… Attach Traffic Manager to the vehicle\n",
    "        self.vehicle.set_autopilot(False, self.tm.get_port())\n",
    "\n",
    "        # âœ… Ensure collision avoidance\n",
    "        self.tm.ignore_lights_percentage(self.vehicle, 100)  # Follow traffic lights\n",
    "        self.tm.ignore_signs_percentage(vehicle, 100)\n",
    "        self.tm.auto_lane_change(self.vehicle, True)  # Initially disable lane changing\n",
    "        self.tm.set_global_distance_to_leading_vehicle(5)  # Avoid collisions\n",
    "        self.tm.vehicle_percentage_speed_difference(self.vehicle, reduce_speed_by)  # 0 speed\n",
    "        self.tm.set_respawn_dormant_vehicles(True)  # Prevent blocked traffic\n",
    "        \n",
    "        #print(\"âœ… Traffic Manager Initialized with collision avoidance.\")\n",
    "        return self.tm  # âœ… Return the Traffic Manager instance\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_route_to_destination(self, new_destination = None):\n",
    "        \"\"\"ðŸ—ºï¸ Computes a route from the vehicle's location to the final destination.\"\"\"\n",
    "        current_location = self.vehicle.get_location()  # âœ… Ensure it's a `Location`\n",
    "        \n",
    "        # âœ… Ensure destination is a `Location`\n",
    "        if new_destination is None :\n",
    "            destination_location = self.destination\n",
    "        \n",
    "        \n",
    "        # âœ… Compute route using CARLAâ€™s Global Route Planner\n",
    "        route = self.route_planner.trace_route(current_location, destination_location)\n",
    "\n",
    "        if not route or len(route) == 0:\n",
    "            #print(\"ðŸš¨ No route generated! Check destination location.\")\n",
    "            return []\n",
    "\n",
    "        #print(f\"ðŸ›£ï¸ Generated Route with {len(route)} waypoints to Destination\")\n",
    "\n",
    "        # âœ… Convert Waypoints to Locations\n",
    "        return [wp[0].transform.location for wp in route]  # âœ… Extract `Location`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def draw_waypoints(self, waypoints, color=carla.Color(0, 255, 0)):\n",
    "        \"\"\"ðŸ—ºï¸ Draws waypoints on the CARLA map (Green Dots ðŸŸ¢).\"\"\"\n",
    "        \n",
    "        world = self.vehicle.get_world()\n",
    "\n",
    "        for wp in waypoints:\n",
    "            world.debug.draw_point(\n",
    "                wp[0].transform.location,\n",
    "                size=0.3,\n",
    "                color=color,\n",
    "                life_time=10  # Stay visible for 10 seconds\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def draw_route(self, waypoints, color=carla.Color(255, 255, 0)):\n",
    "        \"\"\"ðŸ“ Draws lines between waypoints to visualize the full route (Yellow Lines ðŸŸ¡).\"\"\"\n",
    "        \n",
    "        world = self.vehicle.get_world()\n",
    "\n",
    "        for i in range(len(waypoints) - 1):\n",
    "            wp1 = waypoints[i]\n",
    "            wp2 = waypoints[i + 1]\n",
    "\n",
    "            world.debug.draw_line(\n",
    "                wp1, wp2,\n",
    "                thickness=0.1,\n",
    "                color=color,\n",
    "                life_time=0  # Stays visible for 10 seconds\n",
    "            )\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def set_speed(self, desired_speed_kmh):\n",
    "        \"\"\"\n",
    "        Reads the current lane's speed limit, calculates the percentage speed difference,\n",
    "        and applies it to the Traffic Manager so the vehicle aims for `desired_speed_kmh`.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Get the world and traffic manager\n",
    "        world = self.vehicle.get_world()\n",
    "        \n",
    "\n",
    "        # 2) Enable autopilot on this vehicle under the TM's port (if not done yet)\n",
    "        self.vehicle.set_autopilot(True, self.tm.get_port())\n",
    "\n",
    "        # 3) Get the vehicle's current waypoint to find the speed limit\n",
    "        waypoint = world.get_map().get_waypoint(\n",
    "            self.vehicle.get_location(),\n",
    "            project_to_road=True,\n",
    "            lane_type=carla.LaneType.Driving\n",
    "        )\n",
    "\n",
    "        # 4) Read the base speed limit in km/h\n",
    "        base_speed_limit_kmh = self.vehicle.get_speed_limit()  # a float, e.g. 50.0 km/h\n",
    "\n",
    "        # 5) If the base speed limit is zero or negative (rare), just skip\n",
    "        if base_speed_limit_kmh <= 0:\n",
    "            #print(\"ðŸš¨ Invalid speed limit on this road; cannot compute percentage difference.\")\n",
    "            return\n",
    "\n",
    "        # 6) Compute the percentage difference for the Traffic Manager formula:\n",
    "        #    speed = base_speed_limit_kmh * (1 - percentage / 100)\n",
    "        #\n",
    "        # We rearrange:  desired_speed_kmh = base_speed_limit_kmh * (1 - (percentage/100))\n",
    "        # => (percentage/100) = 1 - (desired_speed_kmh / base_speed_limit_kmh)\n",
    "        # => percentage = 100 * (1 - desired_speed_kmh / base_speed_limit_kmh)\n",
    "        #\n",
    "        # Note: if desired_speed_kmh < base_speed_limit_kmh => percentage is positive (slower).\n",
    "        #       if desired_speed_kmh > base_speed_limit_kmh => percentage is negative (faster).\n",
    "        #\n",
    "        ratio = desired_speed_kmh / base_speed_limit_kmh\n",
    "        percentage = 100.0 * (1.0 - ratio)\n",
    "\n",
    "        # 7) Apply the computed percentage to the vehicle\n",
    "        self.tm.vehicle_percentage_speed_difference(self.vehicle, percentage)\n",
    "\n",
    "        # 8) Print debug info\n",
    "        #print(f\"âœ… Base Speed Limit: {base_speed_limit_kmh:.1f} km/h\")\n",
    "        #print(f\"âœ… Desired Speed:    {desired_speed_kmh:.1f} km/h\")\n",
    "        #print(f\"âœ… Computed Pct:     {percentage:.1f}% (positive = slower, negative = faster)\")\n",
    "\n",
    "        # Vehicle will now aim for `desired_speed_kmh` under normal conditions.\n",
    "\n",
    "\n",
    "\n",
    "    def move_to_dwcl(self):\n",
    "        \"\"\"\n",
    "        Moves self.vehicle to the 'dwcl' location using BehaviorAgent and a global route.\n",
    "        \"\"\"\n",
    "        self.vehicle.set_autopilot(False,self.tm.get_port())\n",
    "        #self.tm.vehicle_percentage_speed_difference(self.vehicle, 100)\n",
    "        \n",
    "        agent = CustomBehaviorAgent(self.vehicle, behavior=\"normal\",opt_dict={'ignore_traffic_lights': True})\n",
    "        \n",
    "        #self.disable_autopilot_completely()\n",
    "        self.stop_car()\n",
    "        \n",
    "\n",
    "        # 2. Get the current map and the starting waypoint from the vehicle's position\n",
    "        world_map = self.vehicle.get_world().get_map()\n",
    "        \n",
    "        #time.sleep(0.5)  # Wait for the vehicle to stop\n",
    "\n",
    "        \n",
    "        # 1. Create the BehaviorAgent for your vehicle\n",
    "        #agent = BehaviorAgent(self.vehicle, behavior='normal')  # 'aggressive' or 'cautious' also possible\n",
    "        \n",
    "        \n",
    "\n",
    "        # 3. Retrieve your destination transform\n",
    "        destination_transform = self.get_nearest_dwcl_location()  # Example method returning carla.Transform\n",
    "        if not destination_transform:\n",
    "            #print(\"No destination transform was returned. Aborting.\")\n",
    "            return\n",
    "\n",
    "        # 4. Snap the destination to a lane waypoint so the agent can properly route\n",
    "        end_waypoint = world_map.get_waypoint(\n",
    "            destination_transform.location,\n",
    "            project_to_road=True,\n",
    "            lane_type=carla.LaneType.Driving)\n",
    "        \n",
    "        if not end_waypoint:\n",
    "            #print(\"No valid road waypoint found near destination. Aborting.\")\n",
    "            return\n",
    "        self.vehicle.set_autopilot(False,self.tm.get_port())  # Disable autopilot for manual control\n",
    "        start_waypoint = world_map.get_waypoint(\n",
    "            self.vehicle.get_location(),\n",
    "            project_to_road=True,\n",
    "            lane_type=carla.LaneType.Driving)\n",
    "        \n",
    "        \n",
    "        # 5. Compute a route (list of waypoints) from start to end\n",
    "        route_trace = agent.trace_route(start_waypoint, end_waypoint)\n",
    "        #print('route_trace:',route_trace)\n",
    "        self.draw_waypoints(route_trace)\n",
    "        if len(route_trace) >200:\n",
    "            end_waypoint.transform.location.x = end_waypoint.transform.location.x - end_waypoint.transform.location.x/3\n",
    "            self.vehicle.set_transform(end_waypoint.transform)\n",
    "\n",
    "        # 6. Pass that route to the agent's global planner\n",
    "        agent.set_global_plan(route_trace)\n",
    "        #agent.set_destination(destination_transform.location)\n",
    "\n",
    "        #print(f\"Moving vehicle id={self.vehicle.id} to {destination_transform.location}\")\n",
    "\n",
    "        # 7. Main loop: step the agent until done or timeout\n",
    "        start_time = time.time()\n",
    "        timeout_seconds = 8\n",
    "\n",
    "        while True:\n",
    "            # If you're in asynchronous mode, you can let the simulator run:\n",
    "            #time.sleep(0.05)\n",
    "\n",
    "            # If you're in synchronous mode, use:\n",
    "            #self.vehicle.get_world().tick()\n",
    "            current_time = self.battery_manager.get_simulation_time()\n",
    "            delta_time = current_time - self.battery_manager.last_update_time\n",
    "            self.battery_manager.last_update_time = current_time\n",
    "\n",
    "            self.battery_manager.update_battery(delta_time, self.all_corners)\n",
    "\n",
    "            # 7a. Get the next control from the agent\n",
    "            \n",
    "            control = agent.run_step()\n",
    "            self.vehicle.apply_control(control)\n",
    "            in_dwcl = self.battery_manager.is_on_charging_lane()\n",
    "            current_waypoint = world_map.get_waypoint(\n",
    "            self.vehicle.get_location(),\n",
    "            project_to_road=True,\n",
    "            lane_type=carla.LaneType.Driving)\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            # 7b. Check if the agent has completed the route\n",
    "            if in_dwcl and current_waypoint.lane_id == end_waypoint.lane_id:\n",
    "                # 1. Get the vehicle's current location\n",
    "                \n",
    "                vehicle_location = self.vehicle.get_location()\n",
    "                \n",
    "                # 2. Find the nearest drivable lane waypoint\n",
    "                \n",
    "                nearest_wp = world_map.get_waypoint(\n",
    "                    vehicle_location,\n",
    "                    project_to_road=True, \n",
    "                    lane_type=carla.LaneType.Driving\n",
    "                )\n",
    "                self.stop_car()\n",
    "                # 3. If a valid waypoint is found, move the vehicle to that transform\n",
    "                if nearest_wp is not None:\n",
    "                    \n",
    "                    #print(\"Vehicle teleported to the nearest waypoint on the road.\")\n",
    "                    #for _ in range(26):\n",
    "                     #   self.vehicle.get_world().tick()\n",
    "                    self.stop_car()\n",
    "                    self.vehicle.set_transform(nearest_wp.transform)\n",
    "                else:\n",
    "                    print(\"No valid waypoint found near the vehicleâ€™s location.\")\n",
    "                \n",
    "                # Break out of the driving loop, etc.\n",
    "                break\n",
    "\n",
    "            if self.vehicle.get_location().x > self.destination.x - 5:\n",
    "                self.stop_car()\n",
    "                break\n",
    "                \n",
    "            \n",
    "\n",
    "            # 7c. Check for timeout\n",
    "            if (time.time() - start_time) > timeout_seconds:\n",
    "             #   #print(\"Timeout: Destination not reached within given time.\")\n",
    "                self.vehicle.set_transform(end_waypoint.transform)\n",
    "                break\n",
    "        self.vehicle.set_autopilot(True, self.tm.get_port())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def disable_autopilot_completely(self):\n",
    "        \"\"\"\n",
    "        Fully removes the vehicle from Traffic Manager (TM) \n",
    "        and ensures autopilot is off, even near intersections.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # 2) Unregister (remove) the vehicle from TMâ€™s control\n",
    "        try:\n",
    "            self.tm.unregister_vehicle(self.vehicle)\n",
    "            #print(\"âœ… Vehicle unregistered from Traffic Manager.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ðŸš¨ Could not unregister vehicle from TM: {e}\")\n",
    "\n",
    "        # 3) Disable autopilot (in case it was set before)\n",
    "        self.vehicle.set_autopilot(False)\n",
    "        #print(\"ðŸš¦ Autopilot disabled!\")\n",
    "\n",
    "\n",
    "\n",
    "    def exit_dwcl(self):\n",
    "        \"\"\"\n",
    "        Moves self.vehicle to the 'dwcl' location using BehaviorAgent and a global route.\n",
    "        \"\"\"\n",
    "        self.vehicle.set_autopilot(False,self.tm.get_port())\n",
    "        #self.tm.vehicle_percentage_speed_difference(self.vehicle, 100)\n",
    "        \n",
    "        agent = CustomBehaviorAgent(self.vehicle, behavior=\"normal\",opt_dict={'ignore_traffic_lights': True})\n",
    "        \n",
    "        #self.disable_autopilot_completely()\n",
    "        self.stop_car()\n",
    "        \n",
    "\n",
    "        # 2. Get the current map and the starting waypoint from the vehicle's position\n",
    "        world_map = self.vehicle.get_world().get_map()\n",
    "        \n",
    "        #time.sleep(0.5)  # Wait for the vehicle to stop\n",
    "\n",
    "        \n",
    "        # 1. Create the BehaviorAgent for your vehicle\n",
    "        #agent = BehaviorAgent(self.vehicle, behavior='normal')  # 'aggressive' or 'cautious' also possible\n",
    "        \n",
    "        \n",
    "\n",
    "        # 3. Retrieve your destination transform\n",
    "        destination_transform = self.get_nearest_NOdwcl_location()  # Example method returning carla.Transform\n",
    "        if not destination_transform:\n",
    "            #print(\"No destination transform was returned. Aborting.\")\n",
    "            return\n",
    "\n",
    "        # 4. Snap the destination to a lane waypoint so the agent can properly route\n",
    "        end_waypoint = world_map.get_waypoint(\n",
    "            destination_transform.location,\n",
    "            project_to_road=True,\n",
    "            lane_type=carla.LaneType.Driving)\n",
    "        \n",
    "        if not end_waypoint:\n",
    "            #print(\"No valid road waypoint found near destination. Aborting.\")\n",
    "            return\n",
    "        self.vehicle.set_autopilot(False,self.tm.get_port())  # Disable autopilot for manual control\n",
    "        start_waypoint = world_map.get_waypoint(\n",
    "            self.vehicle.get_location(),\n",
    "            project_to_road=True,\n",
    "            lane_type=carla.LaneType.Driving)\n",
    "        \n",
    "        \n",
    "        # 5. Compute a route (list of waypoints) from start to end\n",
    "        route_trace = agent.trace_route(start_waypoint, end_waypoint)\n",
    "        #print('route_trace:',route_trace)\n",
    "        self.draw_waypoints(route_trace)\n",
    "        if len(route_trace) >200:\n",
    "            end_waypoint.transform.location.x = end_waypoint.transform.location.x - end_waypoint.transform.location.x/3\n",
    "            self.vehicle.set_transform(end_waypoint.transform)\n",
    "\n",
    "        # 6. Pass that route to the agent's global planner\n",
    "        agent.set_global_plan(route_trace)\n",
    "        #agent.set_destination(destination_transform.location)\n",
    "\n",
    "        #print(f\"Moving vehicle id={self.vehicle.id} to {destination_transform.location}\")\n",
    "\n",
    "        # 7. Main loop: step the agent until done or timeout\n",
    "        start_time = time.time()\n",
    "        timeout_seconds = 8\n",
    "        self.tm.vehicle_percentage_speed_difference(self.vehicle, 0)\n",
    "        while True:\n",
    "            # If you're in asynchronous mode, you can let the simulator run:\n",
    "            #time.sleep(0.05)\n",
    "\n",
    "            # If you're in synchronous mode, use:\n",
    "            #self.vehicle.get_world().tick()\n",
    "            current_time = self.battery_manager.get_simulation_time()\n",
    "            delta_time = current_time - self.battery_manager.last_update_time\n",
    "            self.battery_manager.last_update_time = current_time\n",
    "\n",
    "            self.battery_manager.update_battery(delta_time, self.all_corners)\n",
    "\n",
    "            # 7a. Get the next control from the agent\n",
    "            \n",
    "            control = agent.run_step()\n",
    "            self.vehicle.apply_control(control)\n",
    "            in_dwcl = self.battery_manager.is_on_charging_lane()\n",
    "            current_waypoint = world_map.get_waypoint(\n",
    "            self.vehicle.get_location(),\n",
    "            project_to_road=True,\n",
    "            lane_type=carla.LaneType.Driving)\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            # 7b. Check if the agent has completed the route\n",
    "            if not in_dwcl and current_waypoint.lane_id == end_waypoint.lane_id:\n",
    "                # 1. Get the vehicle's current location\n",
    "                \n",
    "                vehicle_location = self.vehicle.get_location()\n",
    "                \n",
    "                # 2. Find the nearest drivable lane waypoint\n",
    "                \n",
    "                nearest_wp = world_map.get_waypoint(\n",
    "                    vehicle_location,\n",
    "                    project_to_road=True, \n",
    "                    lane_type=carla.LaneType.Driving\n",
    "                )\n",
    "                self.stop_car()\n",
    "                # 3. If a valid waypoint is found, move the vehicle to that transform\n",
    "                if nearest_wp is not None:\n",
    "                    \n",
    "                    #print(\"Vehicle teleported to the nearest waypoint on the road.\")\n",
    "                    for _ in range(26):\n",
    "                        self.vehicle.get_world().tick()\n",
    "                    self.stop_car()\n",
    "                    self.vehicle.set_transform(nearest_wp.transform)\n",
    "                else:\n",
    "                    print(\"No valid waypoint found near the vehicleâ€™s location.\")\n",
    "                \n",
    "                # Break out of the driving loop, etc.\n",
    "                break\n",
    "\n",
    "            if self.vehicle.get_location().x > self.destination.x - 5:\n",
    "                self.stop_car()\n",
    "                break\n",
    "                \n",
    "            \n",
    "\n",
    "            # 7c. Check for timeout\n",
    "            if (time.time() - start_time) > timeout_seconds:\n",
    "             #   #print(\"Timeout: Destination not reached within given time.\")\n",
    "                self.vehicle.set_transform(end_waypoint.transform)\n",
    "                break\n",
    "        \n",
    "        self.vehicle.set_autopilot(True, self.tm.get_port())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def exittt_dwcl(self):\n",
    "        \"\"\"ðŸš— Uses Behavior Agent to move left and adjusts vehicle orientation by teleporting to the nearest waypoint.\"\"\"\n",
    "        \n",
    "        #print(\"ðŸ”„ Exiting DWCL by moving to the left lane...\")\n",
    "        self.vehicle.set_autopilot(False,self.tm.get_port())  # Disable autopilot for manual control\n",
    "\n",
    "        # âœ… Get CARLA map\n",
    "        world_map = self.vehicle.get_world().get_map()\n",
    "\n",
    "        # âœ… Initialize Behavior Agent\n",
    "        #agent = BehaviorAgent(self.vehicle, behavior=\"normal\")\n",
    "\n",
    "        # âœ… Get Current Waypoint\n",
    "        \n",
    "\n",
    "        # âœ… Get the Left Lane Waypoint\n",
    "        \n",
    "\n",
    "        #print(f\"ðŸ”„ Moving to left lane {left_wp.lane_id}...\")\n",
    "        agent = CustomBehaviorAgent(self.vehicle, behavior=\"normal\",opt_dict={'ignore_traffic_lights': True})\n",
    "\n",
    "        velocity = vehicle.get_velocity()\n",
    "\n",
    "        # Speed in m/s\n",
    "        speed_m_s = math.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "\n",
    "        # Speed in km/h\n",
    "        speed_kmh = speed_m_s * 3.6\n",
    "        brake_safe_dist = (speed_kmh/7)**2\n",
    "\n",
    "        start_waypoint = world_map.get_waypoint(\n",
    "            self.vehicle.get_location(),\n",
    "            project_to_road=True,\n",
    "            lane_type=carla.LaneType.Driving)\n",
    "        current_wp = world_map.get_waypoint(self.vehicle.get_location(), project_to_road=True,lane_type=carla.LaneType.Driving)\n",
    "        left_wp = current_wp.get_left_lane()\n",
    "        \n",
    "        if not left_wp:\n",
    "            #print(\"ðŸš¨ No left lane available! Staying in current lane.\")\n",
    "            return\n",
    "         \n",
    "        left_wp.transform.location.x -= brake_safe_dist  \n",
    "        \n",
    "\n",
    "        # âœ… Set destination to the left lane\n",
    "        #agent.set_destination(left_wp.transform.location)\n",
    "        route_trace = agent.trace_route(start_waypoint, left_wp)\n",
    "        #print('route_trace:',route_trace)\n",
    "        self.draw_waypoints(route_trace)\n",
    "        if len(route_trace) >200:\n",
    "            left_wp.transform.location.x /= 3\n",
    "            self.vehicle.set_transform(left_wp.transform)\n",
    "\n",
    "        # 6. Pass that route to the agent's global planner\n",
    "        agent.set_global_plan(route_trace)\n",
    "\n",
    "        # âœ… Smoothly follow the planned route\n",
    "        while agent.done() is False:\n",
    "            current_time = self.battery_manager.get_simulation_time()\n",
    "            delta_time = current_time - self.battery_manager.last_update_time\n",
    "            self.battery_manager.last_update_time = current_time\n",
    "            self.battery_manager.update_battery(delta_time, self.all_corners)\n",
    "            start_time = time.time()\n",
    "            timeout_seconds = 8\n",
    "\n",
    "            control = agent.run_step()\n",
    "            self.vehicle.apply_control(control)\n",
    "            time.sleep(0.05)  # Allow smooth execution\n",
    "            current_wp = world_map.get_waypoint(self.vehicle.get_location(), project_to_road=True,lane_type=carla.LaneType.Driving)\n",
    "            if current_wp.lane_id == left_wp.lane_id:\n",
    "                \n",
    "\n",
    "                \n",
    "                new_wp = world_map.get_waypoint(self.vehicle.get_location(), project_to_road=True)\n",
    "                \n",
    "                \n",
    "                self.vehicle.set_transform(new_wp.transform)  # âœ… Teleport to ensure perfect alignment\n",
    "\n",
    "                #print(\"âœ… Vehicle is now perfectly aligned on the left lane!\")\n",
    "                self.stop_car()\n",
    "                break\n",
    "\n",
    "\n",
    "            if (time.time() - start_time) > timeout_seconds:\n",
    "             #   #print(\"Timeout: Destination not reached within given time.\")\n",
    "                self.vehicle.set_transform(left_wp.transform)\n",
    "                break\n",
    "\n",
    "            if self.vehicle.get_location().x > self.destination.x - 5:\n",
    "                self.stop_car()\n",
    "                break\n",
    "        self.tm.vehicle_percentage_speed_difference(self.vehicle, 0)\n",
    "        self.vehicle.set_autopilot(True,self.tm.get_port())  # âœ… Resume autopilot after exiting DWCL\n",
    "\n",
    "    def speed_up(self, delta_v=2):\n",
    "        \n",
    "        self.set_speed(self.target_speed + delta_v)  # Set target speed\n",
    "        self.target_speed += delta_v  # Update target speed\n",
    "\n",
    "    def slow_down(self, delta_v=5):\n",
    "\n",
    "        if self.target_speed - delta_v >= 10:  # Ensure speed is above 5 km/h\n",
    "        \n",
    "            self.set_speed(self.target_speed - delta_v)  # Set target speed\n",
    "            self.target_speed -= delta_v  # Update target speed\n",
    "\n",
    "    def maintain_speed(self):\n",
    "        \n",
    "        self.set_speed(self.target_speed)  # Set target speed\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "    def compute_reward(self, required_soc ,SoC, in_dwcl, travel_time, ETA, speed):\n",
    "        global lane_switch_counter\n",
    "        \"\"\"ðŸ“ˆ Reward Function\"\"\"\n",
    "        reward = 0\n",
    "        if in_dwcl and SoC < required_soc + 20:\n",
    "            reward += 15\n",
    "        if in_dwcl and SoC > 80:\n",
    "            reward -= 10\n",
    "        if in_dwcl and SoC > required_soc + 20:\n",
    "            reward -= 3\n",
    "        if travel_time < (self.battery_manager.initial_eta - ETA) * 1.2:  # ETA correction\n",
    "            reward += 20\n",
    "        if self.target_speed > 50 and in_dwcl:\n",
    "            reward -= 5\n",
    "        if self.target_speed < 15 and in_dwcl:\n",
    "            reward -= 15\n",
    "        if SoC < 20:\n",
    "            reward -= 20\n",
    "        if SoC >= 20:\n",
    "            reward += 20\n",
    "        if self.battery_manager.compute_remaining_distance() < 10 and SoC >= 20:\n",
    "            reward += 30\n",
    "\n",
    "        if self.battery_manager.compute_remaining_distance() < 10 and SoC < 20:\n",
    "            reward -= 30\n",
    "\n",
    "        if lane_switch_counter > 0:\n",
    "            reward -= lane_switch_counter**2\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"ðŸ”„ Execute an action, ensure completion, and return new state, reward, and done flag.\"\"\"\n",
    "        global lane_switch_counter\n",
    "        global invalid_action_counter\n",
    "        global has_arrived\n",
    "        \n",
    "        \n",
    "        #eta = self.battery_manager.eta\n",
    "       # if eta is None:\n",
    "        #    eta = 100  # Assign a high default value if not computed\n",
    "\n",
    "        elapsed_time = self.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds - self.start_time\n",
    "        distance_to_destination = self.battery_manager.compute_remaining_distance()\n",
    "        eta = distance_to_destination/avg_speed\n",
    "        \n",
    "        if self.battery_manager.in_dwcl != self.battery_manager.is_on_charging_lane():\n",
    "        # ðŸ”¹ Get lane information\n",
    "            \n",
    "            lane_switch_counter +=1\n",
    "\n",
    "        self.battery_manager.in_dwcl = self.battery_manager.is_on_charging_lane()\n",
    "        lane_type = int(self.battery_manager.in_dwcl)\n",
    "\n",
    "        # ðŸ”¥ Terminal condition\n",
    "        if self.vehicle.get_location().x > self.destination.x - 5:\n",
    "            print('Simulation Ended;ðŸ”¥ Terminal condition;ðŸ”¥ Terminal condition;ðŸ”¥ Terminal condition;ðŸ”¥ Terminal condition;ðŸ”¥ Terminal condition')\n",
    "            #print('Elapsed time: ', elapsed_time,' Distance to destination: ', distance_to_destination,' Initial eta: ', self.battery_manager.initial_eta)\n",
    "            has_arrived = True\n",
    "            self.stop_car()\n",
    "            return np.array([0, 0, 0, 0, lane_type,0]), 0, True, {}\n",
    "        \n",
    "        if  elapsed_time > 2 * self.battery_manager.initial_eta:\n",
    "            print('Simulation Ended;ðŸ”¥ Terminal condition;ðŸ”¥ Terminal condition;ðŸ”¥ Terminal condition;ðŸ”¥ Terminal condition;ðŸ”¥ Terminal condition')\n",
    "            #print('Elapsed time: ', elapsed_time,' Distance to destination: ', distance_to_destination,' Initial eta: ', self.battery_manager.initial_eta)\n",
    "            has_arrived = False\n",
    "            self.stop_car()\n",
    "            return np.array([0, 0, 0, 0, lane_type,0]), 0, True, {} # End simulation if it takes too long\n",
    "\n",
    "        reward = 0\n",
    "        action_completed = False  # ðŸš€ Track if the action has finished\n",
    "\n",
    "        if action == 0 and lane_type == 0:  # Move to DWCL\n",
    "            #print('Move to DWCL')\n",
    "            self.move_to_dwcl()\n",
    "            action_completed = True\n",
    "            reward += 5\n",
    "        elif action == 1 and lane_type == 1:  # Leave DWCL\n",
    "            #print('Leave DWCL')\n",
    "            self.exit_dwcl()\n",
    "            action_completed = True\n",
    "            reward += 5\n",
    "        elif action == 2 and lane_type == 1:  # Accelerate inside DWCL\n",
    "            #print(' Accelerate inside DWCL')\n",
    "            self.speed_up()\n",
    "            action_completed = True\n",
    "            reward += 5\n",
    "        elif action == 3 and lane_type == 1 and self.target_speed > 10:  # Decelerate inside DWCL #ensure a min of 5 km/h\n",
    "            #print('Decelerate inside DWCL')\n",
    "            self.slow_down()\n",
    "            action_completed = True\n",
    "            reward += 5\n",
    "        elif action == 4 and lane_type == 1:  # Maintain speed inside DWCL\n",
    "            #print('# Maintain speed inside DWCL')\n",
    "            self.maintain_speed()\n",
    "            action_completed = True\n",
    "            reward += 5\n",
    "\n",
    "        elif action == 5 and lane_type == 0:  # Maintain  outside DWCL\n",
    "            #print('# Stay Out Of DWCL')\n",
    "            self.vehicle.set_autopilot(True,self.tm.get_port())\n",
    "            self.tm.vehicle_percentage_speed_difference(self.vehicle, 0)\n",
    "            action_completed = True\n",
    "            reward += 5\n",
    "\n",
    "        else:\n",
    "            #print(\"ðŸš¨ Invalid action for lane type! Skipping action.\")\n",
    "            reward -= 20 # Penalize invalid actions\n",
    "            invalid_action_counter += 1\n",
    "            action_completed = True \n",
    "\n",
    "        velocity = vehicle.get_velocity()\n",
    "        speed_kph = math.sqrt(velocity.x**2 + velocity.y**2) *3.6\n",
    "\n",
    "        current_time = self.battery_manager.get_simulation_time()\n",
    "        delta_time = current_time - self.battery_manager.last_update_time\n",
    "        self.battery_manager.last_update_time = current_time\n",
    "\n",
    "        self.battery_manager.update_battery(delta_time, self.all_corners)\n",
    "\n",
    "        reward += self.compute_reward(self.battery_manager.soc_required,self.battery_manager.soc, lane_type, elapsed_time, eta, speed_kph)\n",
    "\n",
    "        # ðŸ”¹ Wait until action is complete before requesting a new action\n",
    "        while not action_completed:\n",
    "            print(\"â³ Waiting for action completion...\")\n",
    "            #time.sleep(0.5)\n",
    "\n",
    "        #print(f\"âœ… Action {action} completed, requesting next action.\")\n",
    "\n",
    "        # Compute updated state\n",
    "        state = np.array([self.battery_manager.soc, self.battery_manager.soc_required, eta, distance_to_destination, lane_type, self.target_speed], dtype=np.float32)\n",
    "        return state, reward, False, {}\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"ðŸ”„ Fully resets the CARLA simulation and reassigns objects, ensuring the vehicle drives towards its destination.\"\"\"\n",
    "        global lane_switch_counter\n",
    "        global invalid_action_counter\n",
    "        \n",
    "        # âœ… Step 1: Reset the entire CARLA environment\n",
    "        self.vehicle.set_transform(new_start_point)  # Move vehicle to origin\n",
    "        lane_switch_counter = 0\n",
    "        invalid_action_counter = 0\n",
    "\n",
    "        spawn_points = world.get_map().get_spawn_points()\n",
    "        destination = carla.Location(\n",
    "            x=start_point.location.x + length,\n",
    "            y=start_point.location.y,\n",
    "            z=start_point.location.z,\n",
    ")\n",
    "\n",
    "        # âœ… Step 2: Reassign the new objects after reset\n",
    "        self.vehicle = vehicle  # Reassign newly spawned vehicle\n",
    "        self.stop_car()\n",
    "        self.all_corners = all_corners  # Reassign new DWCL coil locations\n",
    "        self.destination = destination  # Reassign destination location\n",
    "\n",
    "        # âœ… Step 3: Reinitialize `BatteryManager` with the new vehicle\n",
    "        self.battery_manager = BatteryManager(self.vehicle, self.destination, world, power_model, C_bat, P_aux)\n",
    "\n",
    "        # âœ… Step 4: Reset simulation time\n",
    "        self.start_time = self.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds  \n",
    "\n",
    "        # âœ… Step 5: Ensure Traffic Manager Leads the Vehicle to Destination\n",
    "        self.set_up_traffic_manager()\n",
    "        #self.initialize_behavior_agent()\n",
    "\n",
    "        # âœ… Step 6: Compute new battery and state information\n",
    "        self.soc, soc_required, _, ETA, _, _, _, _, remaining_distance, _, _ = self.battery_manager.update_battery(0, self.all_corners)\n",
    "\n",
    "        # âœ… Step 7: Check if the vehicle is in DWCL after the reset\n",
    "        self.battery_manager.in_dwcl= self.battery_manager.is_on_charging_lane()\n",
    "        lane_type = int(self.battery_manager.in_dwcl)\n",
    "        self.target_speed = 30\n",
    "\n",
    "        print(\"âœ… Reset complete. Traffic Manager guiding vehicle to destination.\")\n",
    "\n",
    "        # âœ… Step 8: Return initial state for RL training\n",
    "        return np.array([self.soc, soc_required, ETA, remaining_distance, lane_type, self.target_speed], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HOUALEF\\anaconda3\\envs\\carla-sim\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "create_sim_env()\n",
    "# Load power model\n",
    "power_model = load_model(\"c:\\\\Users\\\\HOUALEF\\\\Desktop\\\\RL_Carla\\\\models\\\\energy_model_best.hdf5\")\n",
    "\n",
    "\n",
    "\n",
    "destination = carla.Location(\n",
    "    x=new_start_point.location.x + length,\n",
    "    y=new_start_point.location.y,\n",
    "    z=new_start_point.location.z,\n",
    ")\n",
    "\n",
    "# Initialize battery manager\n",
    "battery_manager = BatteryManager(vehicle, destination, world, power_model,C_bat,P_aux)\n",
    "env = CarlaEnv(vehicle, battery_manager, all_corners, destination)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.23153818402972"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.battery_manager.initial_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"ðŸš€ Neural Network for Q-Learning (DQN)\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    '''ðŸš€ Deeper Neural Network for Q-Learning (DQN)'''\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)  # Increased neurons\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)  # Additional layer\n",
    "        self.fc5 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return self.fc5(x)  # Output layer (no activation)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"ðŸŽ¯ DQN Agent with Experience Replay\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.9, lr=0.001, epsilon=1.0, epsilon_decay=0.99995, epsilon_min=0.05):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.memory = collections.deque(maxlen=10000)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "    def select_action(self, state, evaluate = False):\n",
    "        \"\"\"ðŸ¤– Uses Îµ-greedy policy for action selection\"\"\"\n",
    "        if random.random() < self.epsilon and not evaluate:\n",
    "            return random.randint(0, 5)  # Random action\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.model(state)).item()\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"ðŸ’¾ Store experience in replay buffer\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self, batch_size=64):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # âœ… Debugging: Print each next_state's shape & type\n",
    "        #print(\"\\nðŸ” DEBUG: Checking next_states:\")\n",
    "        #for i, state in enumerate(states):\n",
    "            \n",
    "        #    print(f\"  states[{i}] value: {state}\\n\")\n",
    "\n",
    "        # âœ… Convert to numpy before PyTorch tensor\n",
    "        try:\n",
    "            next_states = [np.array(s, dtype=np.float32) for s in next_states]\n",
    "            next_states = np.array(next_states, dtype=np.float32)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        except Exception as e:\n",
    "            print(\"ðŸš¨ Error while converting next_states:\", e)\n",
    "            print(\"âŒ Debug Info: next_states before conversion:\", next_states)\n",
    "            exit(1)  # Force exit to debug\n",
    "\n",
    "        # âœ… Continue training if no errors\n",
    "        states = torch.tensor(np.array(states, dtype=np.float32), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "    # Compute Q-values, loss, and train the network...\n",
    "\n",
    "\n",
    "        # âœ… Compute Q-values\n",
    "        q_values = self.model(states).gather(1, actions).squeeze(1)\n",
    "        next_q_values = self.model(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # âœ… Optimize\n",
    "        loss = nn.functional.mse_loss(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # âœ… Reduce epsilon for exploration-exploitation tradeoff\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# âœ… Training Metrics Storage\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "epsilon_values = []\n",
    "switch_frequencies = []\n",
    "soc_values = []\n",
    "invalid_action_counts = []\n",
    "arrival_flags = []\n",
    "losses = []\n",
    "times = []  # Initialize with 500 zeros\n",
    "\n",
    "# âœ… Checkpointing Function\n",
    "def save_checkpoint(agent, episode, metrics, filename=\"dqn_checkpoint.pth\"):\n",
    "    \"\"\"Save DQN model, optimizer, and training progress.\"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': agent.model.state_dict(),\n",
    "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "        'epsilon': agent.epsilon,\n",
    "        'episode': episode,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "    # Save metrics as a CSV file\n",
    "    df = pd.DataFrame(metrics)\n",
    "    df.to_csv(\"training_metrics.csv\", mode='w', index=False)  # Overwrite with latest data\n",
    "\n",
    "    print(f\"âœ… Checkpoint saved at Episode {episode}\")\n",
    "\n",
    "# âœ… Load Training Progress\n",
    "def load_checkpoint(agent, filename=\"dqn_checkpoint.pth\"):\n",
    "    \"\"\"Load checkpoint if available and resume training.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        # Restore model and optimizer state\n",
    "        agent.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        agent.epsilon = checkpoint['epsilon']\n",
    "        start_episode = checkpoint['episode']\n",
    "        metrics = checkpoint.get('metrics', {})\n",
    "\n",
    "        print(f\"âœ… Loaded checkpoint from Episode {start_episode}\")\n",
    "\n",
    "        # Load metrics from CSV\n",
    "        if os.path.exists(\"training_metrics.csv\"):\n",
    "            metrics_df = pd.read_csv(\"training_metrics.csv\")\n",
    "            metrics = metrics_df.to_dict(orient=\"list\")\n",
    "\n",
    "        return start_episode, metrics\n",
    "    else:\n",
    "        print(\"ðŸš¨ No checkpoint found. Starting fresh training.\")\n",
    "        return 0, {\"episode_rewards\": [], \"episode_steps\": [], \"epsilon_values\": [],\n",
    "                   \"switch_frequencies\": [], \"soc_values\": [], \"invalid_action_counts\": [], \n",
    "                   \"arrival_flags\": [], \"losses\": []}\n",
    "\n",
    "# âœ… Train the DQN Agent with Automatic Checkpointing\n",
    "def train_dqn(env, agent, num_episodes=500, max_steps=500, batch_size=100, checkpoint_interval=5):\n",
    "    global lane_switch_counter, invalid_action_counter, has_arrived, episode_rewards, episode_steps, epsilon_values\n",
    "    global switch_frequencies, soc_values, invalid_action_counts, arrival_flags, losses, times\n",
    "\n",
    "    # Load previous training state if available\n",
    "    start_episode, metrics = load_checkpoint(agent)\n",
    "    \n",
    "    # Initialize lists from saved metrics\n",
    "    episode_rewards = metrics.get('episode_rewards', [])\n",
    "    episode_steps = metrics.get('episode_steps', [])\n",
    "    epsilon_values = metrics.get('epsilon_values', [])\n",
    "    switch_frequencies = metrics.get('switch_frequencies', [])\n",
    "    soc_values = metrics.get('soc_values', [])\n",
    "    invalid_action_counts = metrics.get('invalid_action_counts', [])\n",
    "    arrival_flags = metrics.get('arrival_flags', [])\n",
    "    losses = metrics.get('losses', [])\n",
    "    times = metrics.get('times', [])\n",
    "\n",
    "    for episode in range(start_episode, num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        episode_time = env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds\n",
    "\n",
    "        while not done and step_count < max_steps:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            loss = agent.train(batch_size)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "        # Store metrics\n",
    "        end_time = env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds\n",
    "        episode_time = end_time - episode_time\n",
    "        \n",
    "        times.append(episode_time)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(step_count)\n",
    "        epsilon_values.append(agent.epsilon)\n",
    "        switch_frequencies.append(lane_switch_counter)\n",
    "        soc_values.append(env.battery_manager.soc)\n",
    "        invalid_action_counts.append(invalid_action_counter)\n",
    "        arrival_flags.append(has_arrived)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(f\"Episode {episode+1}/{num_episodes} - Reward: {episode_reward}, Steps: {step_count}, \"\n",
    "              f\"Epsilon: {agent.epsilon:.2f}, Switch_Freq: {lane_switch_counter}, SOC: {env.battery_manager.soc:.2f}, \"\n",
    "              f\"Invalid Actions: {invalid_action_counter}, Arrived: {has_arrived}, Time: {episode_time:.2f}s\")\n",
    "\n",
    "        # âœ… Save checkpoint every 50 episodes\n",
    "        if (episode + 1) % checkpoint_interval == 0:\n",
    "            metrics = {\n",
    "                \"episode_rewards\": episode_rewards,\n",
    "                \"episode_steps\": episode_steps,\n",
    "                \"epsilon_values\": epsilon_values,\n",
    "                \"switch_frequencies\": switch_frequencies,\n",
    "                \"soc_values\": soc_values,\n",
    "                \"invalid_action_counts\": invalid_action_counts,\n",
    "                \"arrival_flags\": arrival_flags,\n",
    "                \"losses\": losses,\n",
    "                \"times\": times\n",
    "            }\n",
    "            save_checkpoint(agent, episode + 1, metrics)\n",
    "\n",
    "    print(\"âœ… Training complete.\")\n",
    "    plot_training_metrics()\n",
    "\n",
    "# âœ… Plot Training Metrics\n",
    "\n",
    "\n",
    "def smooth_curve(data, window_size=10):\n",
    "    \"\"\"Applies a moving average filter to smooth data curves.\"\"\"\n",
    "    if len(data) < window_size:\n",
    "        return data  # Return raw data if not enough points for smoothing\n",
    "    return pd.Series(data).rolling(window=window_size, min_periods=1, center=True).mean()\n",
    "\n",
    "def plot_training_metrics():\n",
    "    \"\"\"Plot smooth curves for episode rewards, steps, epsilon, switch frequency, SOC, invalid actions, and arrival flags.\"\"\"\n",
    "    threshold = 1400\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(14, 10))\n",
    "\n",
    "    axs[0, 0].plot(smooth_curve(episode_rewards[threshold:]), label=\"Episode Reward\")\n",
    "    axs[0, 0].set_title(\"Episode Rewards\")\n",
    "    axs[0, 0].set_xlabel(\"Episode\")\n",
    "    axs[0, 0].set_ylabel(\"Reward\")\n",
    "\n",
    "    axs[0, 1].plot(smooth_curve(losses[threshold:]), label=\"Loss per Episode\", color=\"orange\")\n",
    "    axs[0, 1].set_title(\"Loss per Episode\")\n",
    "    axs[0, 1].set_xlabel(\"Episode\")\n",
    "    axs[0, 1].set_ylabel(\"Loss (MSE)\")\n",
    "\n",
    "    axs[0, 2].plot(smooth_curve(epsilon_values[threshold:]), label=\"Epsilon Decay\", color=\"green\")\n",
    "    axs[0, 2].set_title(\"Epsilon Decay\")\n",
    "    axs[0, 2].set_xlabel(\"Episode\")\n",
    "    axs[0, 2].set_ylabel(\"Epsilon\")\n",
    "\n",
    "    axs[1, 0].plot(smooth_curve(switch_frequencies[threshold:]), label=\"Lane Switch Frequency\", color=\"red\")\n",
    "    axs[1, 0].set_title(\"Lane Switch Frequency\")\n",
    "    axs[1, 0].set_xlabel(\"Episode\")\n",
    "    axs[1, 0].set_ylabel(\"Switches\")\n",
    "\n",
    "    axs[1, 1].plot(smooth_curve(soc_values[threshold:]), label=\"Battery SOC\", color=\"purple\")\n",
    "    axs[1, 1].set_title(\"Battery SOC at End of Episode\")\n",
    "    axs[1, 1].set_xlabel(\"Episode\")\n",
    "    axs[1, 1].set_ylabel(\"SOC (%)\")\n",
    "\n",
    "    axs[1, 2].plot(smooth_curve(invalid_action_counts[threshold:]), label=\"Invalid Actions\", color=\"brown\")\n",
    "    axs[1, 2].set_title(\"Invalid Actions per Episode\")\n",
    "    axs[1, 2].set_xlabel(\"Episode\")\n",
    "    axs[1, 2].set_ylabel(\"Count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store episode metrics\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "epsilon_values = []\n",
    "switch_frequencies = []\n",
    "soc_values = []\n",
    "invalid_action_counts = []\n",
    "arrival_flags = []\n",
    "losses = []\n",
    "\n",
    "def train_dqn(env, agent, num_episodes=500, max_steps=500, batch_size=100):\n",
    "    global lane_switch_counter, invalid_action_counter, has_arrived\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "\n",
    "        while not done and step_count < max_steps:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            loss = agent.train(batch_size)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "        # Store metrics for plotting\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(step_count)\n",
    "        epsilon_values.append(agent.epsilon)\n",
    "        switch_frequencies.append(lane_switch_counter)\n",
    "        soc_values.append(env.battery_manager.soc)\n",
    "        invalid_action_counts.append(invalid_action_counter)\n",
    "        arrival_flags.append(has_arrived)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(f\"Episode {episode+1}/{num_episodes} - Reward: {episode_reward}, Steps: {step_count}, \"\n",
    "              f\"Epsilon: {agent.epsilon:.2f}, Switch_Freq: {lane_switch_counter}, SOC: {env.battery_manager.soc:.2f}, \"\n",
    "              f\"Invalid Actions: {invalid_action_counter}, Arrived: {has_arrived}\")\n",
    "\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            torch.save(agent.model.state_dict(), f\"dqn_carla_episode_{episode+1}.pth\")\n",
    "\n",
    "    # Plot the metrics after training\n",
    "    plot_training_metrics()\n",
    "\n",
    "def plot_training_metrics():\n",
    "    ''' Plot episode rewards, steps, epsilon, switch frequency, SOC, invalid actions, and arrival flags. '''\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 2, figsize=(12, 10))\n",
    "    \n",
    "    axs[0, 0].plot(episode_rewards, label=\"Episode Reward\")\n",
    "    axs[0, 0].set_title(\"Episode Rewards\")\n",
    "    axs[0, 0].set_xlabel(\"Episode\")\n",
    "    axs[0, 0].set_ylabel(\"Reward\")\n",
    "\n",
    "    axs[0, 1].plot(losses, label=\"loss per Episode\", color=\"orange\")\n",
    "    axs[0, 1].set_title(\"Loss per Episode\")\n",
    "    axs[0, 1].set_xlabel(\"Episode\")\n",
    "    axs[0, 1].set_ylabel(\"Loss (MSE)\")\n",
    "\n",
    "    axs[1, 0].plot(epsilon_values, label=\"Epsilon Decay\", color=\"green\")\n",
    "    axs[1, 0].set_title(\"Epsilon Decay\")\n",
    "    axs[1, 0].set_xlabel(\"Episode\")\n",
    "    axs[1, 0].set_ylabel(\"Epsilon\")\n",
    "\n",
    "    axs[1, 1].plot(switch_frequencies, label=\"Lane Switch Frequency\", color=\"red\")\n",
    "    axs[1, 1].set_title(\"Lane Switch Frequency\")\n",
    "    axs[1, 1].set_xlabel(\"Episode\")\n",
    "    axs[1, 1].set_ylabel(\"Switches\")\n",
    "\n",
    "    axs[2, 0].plot(soc_values, label=\"Battery SOC\", color=\"purple\")\n",
    "    axs[2, 0].set_title(\"Battery SOC at End of Episode\")\n",
    "    axs[2, 0].set_xlabel(\"Episode\")\n",
    "    axs[2, 0].set_ylabel(\"SOC (%)\")\n",
    "\n",
    "    axs[2, 1].plot(invalid_action_counts, label=\"Invalid Actions\", color=\"brown\")\n",
    "    axs[2, 1].set_title(\"Invalid Actions per Episode\")\n",
    "    axs[2, 1].set_xlabel(\"Episode\")\n",
    "    axs[2, 1].set_ylabel(\"Count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "create_sim_env()\n",
    "# Load power model\n",
    "power_model = load_model(\"c:\\\\Users\\\\HOUALEF\\\\Desktop\\\\RL_Carla\\\\models\\\\energy_model_best.hdf5\")\n",
    "\n",
    "spawn_points = world.get_map().get_spawn_points()\n",
    "destination = carla.Location(\n",
    "    x=start_point.location.x + length,\n",
    "    y=start_point.location.y,\n",
    "    z=start_point.location.z,\n",
    ")\n",
    "\n",
    "# Initialize battery manager\n",
    "battery_manager = BatteryManager(vehicle, destination, world, power_model,C_bat,P_aux)\n",
    "env = CarlaEnv(vehicle, battery_manager, all_corners, destination)\n",
    "\n",
    "# Create DQN agent\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "# Start training\n",
    "train_dqn(env, agent, num_episodes=6500, max_steps=500)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define range for plotting\n",
    "start_episode = 1400\n",
    "plot_episodes = np.arange(len(epsilon_values[start_episode:]))\n",
    "\n",
    "sns.set_palette(\"Set2\")  # Using Set2 for more distinguishable colors\n",
    "colors_rl = [\"#1f77b4\", \"#aec7e8\"]  # RL: Blue shades\n",
    "colors_rule = [\"#ff7f0e\", \"#ffbb78\"]  # Rule: Orange shades\n",
    "\n",
    "# Function to smooth the epsilon values\n",
    "def smooth_curve(data, window_size=20):\n",
    "    return pd.Series(data).rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "# Apply smoothing\n",
    "smoothing_window = 30\n",
    "smoothed_epsilon = smooth_curve(epsilon_values[start_episode:], window_size=smoothing_window)\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Color gradient for decay\n",
    "colors = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "\n",
    "# Plot smoothed epsilon decay\n",
    "plt.plot(plot_episodes, smoothed_epsilon, label=\" Epsilon Decay\", color=colors_rule[0], linewidth=2)\n",
    "\n",
    "# Optional: Add a confidence interval\n",
    "std_dev = np.std(epsilon_values[start_episode:])  # Compute standard deviation\n",
    "lower_bound = np.maximum(smoothed_epsilon - std_dev, 0)  # Ensure non-negative values\n",
    "upper_bound = smoothed_epsilon + std_dev\n",
    "#plt.fill_between(plot_episodes, lower_bound, upper_bound, alpha=0.2, color=colors(0.2), label=\"Confidence Interval\")\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel(\"Episode\", fontsize=12, fontweight=\"bold\" )\n",
    "plt.ylabel(\"Epsilon Value\", fontsize=12, fontweight=\"bold\")\n",
    "#plt.title(\"Epsilon Decay Over Episodes\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Grid and legend\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# âœ… Run 100 Test Episodes for RL-Based Agent\n",
    "def evaluate_rl_agent(env, agent, num_episodes=100, max_steps=500):\n",
    "    soc_results = []\n",
    "    travel_times = []\n",
    "    lane_switch_counts = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        lane_switch_counter = 0\n",
    "        start_time = env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds\n",
    "\n",
    "        while not done and step_count < max_steps:\n",
    "            prev_in_dwcl = env.battery_manager.in_dwcl\n",
    "            action = agent.select_action(state)\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "\n",
    "            # âœ… Detect lane switch\n",
    "            if prev_in_dwcl != env.battery_manager.is_on_charging_lane():\n",
    "                lane_switch_counter += 1\n",
    "            env.battery_manager.in_dwcl = env.battery_manager.is_on_charging_lane()\n",
    "\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "\n",
    "        # âœ… Record final SOC, travel time, and lane switches\n",
    "        final_soc = env.battery_manager.soc\n",
    "        end_time = env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds\n",
    "        total_travel_time = end_time - start_time\n",
    "\n",
    "        soc_results.append(final_soc)\n",
    "        travel_times.append(total_travel_time)\n",
    "        lane_switch_counts.append(lane_switch_counter)\n",
    "\n",
    "        print(f\"[RL-Agent] Episode {episode+1}/{num_episodes} - Final SOC: {final_soc:.2f}% - Travel Time: {total_travel_time:.2f}s - Lane Switches: {lane_switch_counter}\")\n",
    "\n",
    "    return soc_results, travel_times, lane_switch_counts\n",
    "\n",
    "\n",
    "# âœ… Run 100 Test Episodes for Rule-Based Baseline\n",
    "def evaluate_rule_based(env, num_episodes=100, max_steps=500):\n",
    "    soc_results = []\n",
    "    travel_times = []\n",
    "    lane_switch_counts = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        lane_switch_counter = 0\n",
    "        start_time = env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds\n",
    "\n",
    "        while not done and step_count < max_steps:\n",
    "            prev_in_dwcl = env.battery_manager.in_dwcl\n",
    "            soc = env.battery_manager.soc\n",
    "            soc_required = env.battery_manager.soc_required\n",
    "\n",
    "            # ðŸŽ¯ Rule-Based Decision Making\n",
    "            if soc < soc_required + 20:\n",
    "                action = 0  # Move to DWCL\n",
    "            elif soc >= soc_required + 20:\n",
    "                action = 1  # Leave DWCL\n",
    "                env.tm.vehicle_percentage_speed_difference(vehicle, 0)\n",
    "            else:\n",
    "                action = 5  # Maintain speed outside DWCL\n",
    "                env.tm.vehicle_percentage_speed_difference(vehicle, 0)\n",
    "\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "\n",
    "            # âœ… Detect lane switch\n",
    "            if prev_in_dwcl != env.battery_manager.is_on_charging_lane():\n",
    "                lane_switch_counter += 1\n",
    "            env.battery_manager.in_dwcl = env.battery_manager.is_on_charging_lane()\n",
    "\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "\n",
    "        # âœ… Record final SOC, travel time, and lane switches\n",
    "        final_soc = env.battery_manager.soc\n",
    "        end_time = env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds\n",
    "        total_travel_time = end_time - start_time\n",
    "\n",
    "        soc_results.append(final_soc)\n",
    "        travel_times.append(total_travel_time)\n",
    "        lane_switch_counts.append(lane_switch_counter)\n",
    "\n",
    "        print(f\"[Rule-Based] Episode {episode+1}/{num_episodes} - Final SOC: {final_soc:.2f}% - Travel Time: {total_travel_time:.2f}s - Lane Switches: {lane_switch_counter}\")\n",
    "\n",
    "    return soc_results, travel_times, lane_switch_counts\n",
    "\n",
    "\n",
    "# âœ… Plot Comparison Between RL and Rule-Based Model\n",
    "def plot_comparison(rl_soc, rl_time, rl_switches, rule_soc, rule_time, rule_switches):\n",
    "    sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Final SOC Comparison\n",
    "    sns.boxplot(data=[rl_soc, rule_soc], ax=axs[0], showmeans=True, palette=[\"blue\", \"orange\"])\n",
    "    axs[0].set_xticklabels([\"RL-Based\", \"Rule-Based\"])\n",
    "    axs[0].set_title(\"Final SOC at Destination\")\n",
    "    axs[0].set_ylabel(\"SOC (%)\")\n",
    "\n",
    "    # Travel Time Comparison\n",
    "    sns.boxplot(data=[rl_time, rule_time], ax=axs[1], showmeans=True, palette=[\"blue\", \"orange\"])\n",
    "    axs[1].set_xticklabels([\"RL-Based\", \"Rule-Based\"])\n",
    "    axs[1].set_title(\"Total Travel Time\")\n",
    "    axs[1].set_ylabel(\"Time (seconds)\")\n",
    "\n",
    "    # Lane Switches Comparison\n",
    "    sns.boxplot(data=[rl_switches, rule_switches], ax=axs[2], showmeans=True, palette=[\"blue\", \"orange\"])\n",
    "    axs[2].set_xticklabels([\"RL-Based\", \"Rule-Based\"])\n",
    "    axs[2].set_title(\"Total Lane Switches\")\n",
    "    axs[2].set_ylabel(\"Count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# âœ… Perform Statistical Tests for Significance\n",
    "def run_stat_tests(data1, data2, metric_name):\n",
    "    ''' Performs Mann-Whitney U Test to compare RL vs Rule-Based results '''\n",
    "    stat, p_value = stats.mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "    print(f\"ðŸ“Š Statistical Test for {metric_name}:\")\n",
    "    print(f\"U-Statistic: {stat:.2f}, P-Value: {p_value:.6f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(f\"âœ… Significant difference found for {metric_name} (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"âŒ No significant difference found for {metric_name} (p >= 0.05)\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "\n",
    "# âœ… Run Evaluations for Both Models\n",
    "rl_soc, rl_time, rl_switches = evaluate_rl_agent(env, agent,num_episodes=1)\n",
    "rule_soc, rule_time, rule_switches = evaluate_rule_based(env,num_episodes=10)\n",
    "\n",
    "# âœ… Plot Results\n",
    "plot_comparison(rl_soc, rl_time, rl_switches, rule_soc, rule_time, rule_switches)\n",
    "\n",
    "# âœ… Run Statistical Tests\n",
    "run_stat_tests(rl_soc, rule_soc, \"Final SOC\")\n",
    "run_stat_tests(rl_time, rule_time, \"Total Travel Time\")\n",
    "run_stat_tests(rl_switches, rule_switches, \"Total Lane Switches\")\"\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_policy(env, agent, num_episodes=100, use_rl=True, initial_soc=19.0):\n",
    "    \"\"\"\n",
    "    Evaluates the RL agent or Rule-Based approach over multiple test episodes.\n",
    "    \n",
    "    Tracks:\n",
    "    - Final SOC\n",
    "    - Total Travel Time (Simulation Time)\n",
    "    - Total Lane Switches\n",
    "    - Time Spent Inside DWCL (Simulation Time)\n",
    "    - Time Spent Outside DWCL (Simulation Time)\n",
    "    \"\"\"\n",
    "\n",
    "    final_soc_list = []\n",
    "    travel_time_list = []\n",
    "    lane_switch_list = []\n",
    "    time_inside_dwcl_list = []\n",
    "    time_outside_dwcl_list = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        # Initialize time tracking from simulation timestamp\n",
    "        start_time = env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds\n",
    "        time_inside_dwcl = 0\n",
    "        time_outside_dwcl = 0\n",
    "        lane_switches = 0\n",
    "\n",
    "        in_dwcl = env.battery_manager.is_on_charging_lane()  # Initial DWCL status\n",
    "\n",
    "        while not done:\n",
    "            current_time = env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds  # Get current sim time\n",
    "\n",
    "            if use_rl:\n",
    "                action = agent.select_action(state, evaluate=True)\n",
    "            else:\n",
    "                # Rule-Based Policy: Enter DWCL if SOC < SOC_Required + 20, Exit if SOC > SOC_Required + 20\n",
    "                soc = env.battery_manager.soc\n",
    "                required_soc = env.battery_manager.soc_required + 20\n",
    "                if in_dwcl and soc > required_soc:\n",
    "                    action = 1  # Exit DWCL\n",
    "                elif not in_dwcl and soc < required_soc:\n",
    "                    action = 0  # Enter DWCL\n",
    "                else:\n",
    "                    action = 5  # Maintain current lane\n",
    "\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "\n",
    "            # Track lane switches\n",
    "            new_in_dwcl = env.battery_manager.is_on_charging_lane()\n",
    "            if in_dwcl != new_in_dwcl:\n",
    "                lane_switches += 1\n",
    "\n",
    "            # Update time spent in/out of DWCL\n",
    "            if new_in_dwcl:\n",
    "                time_inside_dwcl += env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds - current_time\n",
    "            else:\n",
    "                time_outside_dwcl += env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds - current_time\n",
    "\n",
    "            # Update state and lane status\n",
    "            state = next_state\n",
    "            in_dwcl = new_in_dwcl\n",
    "\n",
    "        # Compute final simulation time\n",
    "        total_time = env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds - start_time\n",
    "\n",
    "        # Store results\n",
    "        final_soc_list.append(env.battery_manager.soc)\n",
    "        travel_time_list.append(total_time)\n",
    "        lane_switch_list.append(lane_switches)\n",
    "        time_inside_dwcl_list.append(time_inside_dwcl)\n",
    "        time_outside_dwcl_list.append(time_outside_dwcl)\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{num_episodes} - Final SOC: {env.battery_manager.soc:.2f}, \"\n",
    "              f\"Travel Time: {total_time:.2f}s, Lane Switches: {lane_switches}, \"\n",
    "              f\"Time Inside DWCL: {time_inside_dwcl:.2f}s, Time Outside DWCL: {time_outside_dwcl:.2f}s\")\n",
    "\n",
    "    return {\n",
    "        \"final_soc\": np.array(final_soc_list),\n",
    "        \"travel_time\": np.array(travel_time_list),\n",
    "        \"lane_switches\": np.array(lane_switch_list),\n",
    "        \"time_inside_dwcl\": np.array(time_inside_dwcl_list),\n",
    "        \"time_outside_dwcl\": np.array(time_outside_dwcl_list)\n",
    "    }\n",
    "\n",
    "\n",
    "# **Run evaluation for both RL and Rule-Based models**\n",
    "num_test_episodes = 1\n",
    "\n",
    "print(\"\\nEvaluating RL-Based Model...\")\n",
    "rl_results = evaluate_policy(env, agent, num_test_episodes, use_rl=True)\n",
    "\n",
    "print(\"\\nEvaluating Rule-Based Model...\")\n",
    "rule_results = evaluate_policy(env, agent, num_test_episodes, use_rl=False)\n",
    "\n",
    "\n",
    "# **Plot results in comparison charts**\n",
    "def plot_comparisons(rl_data, rule_data, metric_name, ylabel):\n",
    "    \"\"\"Plots a comparison of RL vs Rule-Based performance.\"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar([\"RL-Based\", \"Rule-Based\"], [np.mean(rl_data), np.mean(rule_data)], color=[\"blue\", \"orange\"])\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f\"Comparison of {metric_name} (Averaged Over {num_test_episodes} Episodes)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot Final SOC\n",
    "plot_comparisons(rl_results[\"final_soc\"], rule_results[\"final_soc\"], \"Final SOC\", \"Final SOC (%)\")\n",
    "\n",
    "# Plot Travel Time\n",
    "plot_comparisons(rl_results[\"travel_time\"], rule_results[\"travel_time\"], \"Total Travel Time\", \"Simulation Time (s)\")\n",
    "\n",
    "# Plot Lane Switches\n",
    "plot_comparisons(rl_results[\"lane_switches\"], rule_results[\"lane_switches\"], \"Total Lane Switches\", \"Number of Switches\")\n",
    "\n",
    "# Plot Time Inside DWCL\n",
    "plot_comparisons(rl_results[\"time_inside_dwcl\"], rule_results[\"time_inside_dwcl\"], \"Time Inside DWCL\", \"Simulation Time in DWCL (s)\")\n",
    "\n",
    "# Plot Time Outside DWCL\n",
    "plot_comparisons(rl_results[\"time_outside_dwcl\"], rule_results[\"time_outside_dwcl\"], \"Time Outside DWCL\", \"Simulation Time Outside DWCL (s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "speed_inside_dwcl = []\n",
    "speed_outsite_dwcl = []\n",
    "def evaluate_policy(env, agent, num_episodes=100, use_rl=True, initial_soc=19.0):\n",
    "    \"\"\"\n",
    "    Evaluates the RL agent or Rule-Based approach over multiple test episodes for a given initial SoC.\n",
    "    \n",
    "    Tracks:\n",
    "    - Final SOC\n",
    "    - Total Travel Time (Simulation Time)\n",
    "    - Total Lane Switches\n",
    "    - Time Spent Inside DWCL (Simulation Time)\n",
    "    \"\"\"\n",
    "    global speed_inside_dwcl\n",
    "    global speed_outsite_dwcl\n",
    "    final_soc_list = []\n",
    "    travel_time_list = []\n",
    "    lane_switch_list = []\n",
    "    time_inside_dwcl_list = []\n",
    "    \n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment with specified initial SOC\n",
    "        \n",
    "        state = env.reset()\n",
    "\n",
    "        env.battery_manager.soc = initial_soc  \n",
    "        done = False\n",
    "        \n",
    "        # Initialize time tracking from simulation timestamp\n",
    "        start_time = env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds\n",
    "        time_inside_dwcl = 0\n",
    "        lane_switches = 0\n",
    "\n",
    "        in_dwcl = env.battery_manager.is_on_charging_lane()  # Initial DWCL status\n",
    "\n",
    "        while not done:\n",
    "            current_time = env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds  # Get current sim time\n",
    "\n",
    "            if use_rl:\n",
    "                \n",
    "                action = agent.select_action(state, evaluate=True)\n",
    "                velocity = env.vehicle.get_velocity()\n",
    "                speed_kmh = math.sqrt(velocity.x**2 + velocity.y**2)*3.6\n",
    "                #print(speed_kmh)\n",
    "                speed_inside_dwcl.append(speed_kmh)\n",
    "            else:\n",
    "                # Rule-Based Policy: Enter DWCL if SOC < SOC_Required + 20, Exit if SOC > SOC_Required + 20\n",
    "                soc = env.battery_manager.soc\n",
    "                required_soc = env.battery_manager.soc_required + 20\n",
    "                if in_dwcl and soc > required_soc:\n",
    "                    action = 1  # Exit DWCL\n",
    "                    \n",
    "                elif not in_dwcl and soc < required_soc:\n",
    "                    \n",
    "                    action = 0  # Enter DWCL\n",
    "\n",
    "               \n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    action = 5  # Maintain current lane\n",
    "                    \n",
    "\n",
    "                velocity = env.vehicle.get_velocity()\n",
    "                speed_kmh = math.sqrt(velocity.x**2 + velocity.y**2)*3.6\n",
    "                #print(speed_kmh)\n",
    "                speed_outsite_dwcl.append(speed_kmh)\n",
    "\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "\n",
    "            # Track lane switches\n",
    "            new_in_dwcl = env.battery_manager.is_on_charging_lane()\n",
    "            if in_dwcl != new_in_dwcl:\n",
    "                lane_switches += 1\n",
    "\n",
    "            # Update time spent in DWCL\n",
    "            if new_in_dwcl:\n",
    "                time_inside_dwcl += env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds - current_time\n",
    "\n",
    "            # Update state and lane status\n",
    "            state = next_state\n",
    "            in_dwcl = new_in_dwcl\n",
    "\n",
    "        # Compute final simulation time\n",
    "        total_time = env.vehicle.get_world().get_snapshot().timestamp.elapsed_seconds - start_time\n",
    "\n",
    "        # Store results\n",
    "        final_soc_list.append(env.battery_manager.soc)\n",
    "        travel_time_list.append(total_time)\n",
    "        lane_switch_list.append(lane_switches)\n",
    "        time_inside_dwcl_list.append(time_inside_dwcl)\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{num_episodes} | SOC: {env.battery_manager.soc:.2f}, \"\n",
    "              f\"Time: {total_time:.2f}s, Switches: {lane_switches}, Inside DWCL: {time_inside_dwcl:.2f}s\")\n",
    "\n",
    "    return {\n",
    "        \"final_soc\": np.array(final_soc_list),\n",
    "        \"travel_time\": np.array(travel_time_list),\n",
    "        \"lane_switches\": np.array(lane_switch_list),\n",
    "        \"time_inside_dwcl\": np.array(time_inside_dwcl_list),\n",
    "    }\n",
    "\n",
    "\n",
    "# **Run evaluation for different SoC initial conditions**\n",
    "num_test_episodes = 10\n",
    "initial_socs = [17.2, 20.7]  # Different initial conditions\n",
    "\n",
    "results = {\"RL\": {}, \"Rule\": {}}\n",
    "\n",
    "for soc in initial_socs:\n",
    "    print(f\"\\nEvaluating RL-Based Model (SoC={soc})...\")\n",
    "    results[\"RL\"][soc] = evaluate_policy(env, agent, num_test_episodes, use_rl=True, initial_soc=soc)\n",
    "\n",
    "    print(f\"\\nEvaluating Rule-Based Model (SoC={soc})...\")\n",
    "    results[\"Rule\"][soc] = evaluate_policy(env, agent, num_test_episodes, use_rl=False, initial_soc=soc)\n",
    "\n",
    "\n",
    "# **Visualization: Horizontal Subplots with Vertical Bars**\n",
    "def plot_comparison_bars(results, metrics, titles, ylabels):\n",
    "    \"\"\"Creates horizontally arranged subplots comparing RL vs Rule-Based performance across scenarios.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(18, 5))\n",
    "\n",
    "    soc_values = list(results[\"RL\"].keys())\n",
    "\n",
    "    for i, (metric, title, ylabel) in enumerate(zip(metrics, titles, ylabels)):\n",
    "        rl_means = [np.mean(results[\"RL\"][soc][metric]) for soc in soc_values]\n",
    "        rule_means = [np.mean(results[\"Rule\"][soc][metric]) for soc in soc_values]\n",
    "\n",
    "        x = np.arange(len(soc_values))\n",
    "        width = 0.35\n",
    "\n",
    "        # Vertical bars\n",
    "        axes[i].bar(x - width/2, rl_means, width, label=\"RL-Based\", color=\"royalblue\")\n",
    "        axes[i].bar(x + width/2, rule_means, width, label=\"Rule-Based\", color=\"darkorange\")\n",
    "\n",
    "        axes[i].set_ylabel(ylabel)\n",
    "        axes[i].set_xticks(x)\n",
    "        axes[i].set_xticklabels([f\"SoC={s}\" for s in soc_values])\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# **Plot results for all three metrics**\n",
    "metrics = [\"final_soc\", \"travel_time\", \"lane_switches\"]\n",
    "titles = [\"Final SOC Across Scenarios\", \"Total Travel Time & DWCL Time\", \"Lane Switches Across Scenarios\"]\n",
    "ylabels = [\"Final SOC (%)\", \"Time (s)\", \"Number of Switches\"]\n",
    "\n",
    "plot_comparison_bars(results, metrics, titles, ylabels)\n",
    "\n",
    "\n",
    "# **Plot Combined Travel Time & DWCL Time**\n",
    "def plot_travel_and_dwcl_time(results):\n",
    "    \"\"\"Plots total travel time with time inside DWCL as a stacked bar representation for different scenarios.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    soc_values = list(results[\"RL\"].keys())\n",
    "    rl_total_time = [np.mean(results[\"RL\"][soc][\"travel_time\"]) for soc in soc_values]\n",
    "    rl_inside_dwcl = [np.mean(results[\"RL\"][soc][\"time_inside_dwcl\"]) for soc in soc_values]\n",
    "    rule_total_time = [np.mean(results[\"Rule\"][soc][\"travel_time\"]) for soc in soc_values]\n",
    "    rule_inside_dwcl = [np.mean(results[\"Rule\"][soc][\"time_inside_dwcl\"]) for soc in soc_values]\n",
    "\n",
    "    x = np.arange(len(soc_values))\n",
    "    width = 0.35\n",
    "\n",
    "    # Stacked bars for RL and Rule-Based models\n",
    "    ax.bar(x - width/2, rl_total_time, width, label=\"RL Total Travel Time\", color=\"dodgerblue\", alpha=0.8)\n",
    "    ax.bar(x - width/2, rl_inside_dwcl, width, label=\"RL Inside DWCL\", color=\"navy\")\n",
    "\n",
    "    ax.bar(x + width/2, rule_total_time, width, label=\"Rule Total Travel Time\", color=\"orangered\", alpha=0.8)\n",
    "    ax.bar(x + width/2, rule_inside_dwcl, width, label=\"Rule Inside DWCL\", color=\"darkred\")\n",
    "\n",
    "    ax.set_ylabel(\"Simulation Time (s)\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f\"SoC={s}\" for s in soc_values])\n",
    "    ax.set_title(\"Total Travel Time and Time Spent in DWCL\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_travel_and_dwcl_time(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(speed_inside_dwcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(speed_inside_dwcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(speed_outsite_dwcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(speed_outsite_dwcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to apply rolling smoothing\n",
    "def smooth_curve(data, window_size=20):\n",
    "    return pd.Series(data).rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "# Function to compute rolling standard deviation\n",
    "def compute_rolling_std(data, window_size=20):\n",
    "    return pd.Series(data).rolling(window=window_size, min_periods=1).std()\n",
    "\n",
    "# Convert speed data to NumPy array\n",
    "speed_inside_dwcl = np.array(speed_inside_dwcl)\n",
    "\n",
    "# Define smoothing window\n",
    "smoothing_window = 5  # Change this to control smoothness\n",
    "\n",
    "# Apply rolling smoothing\n",
    "smoothed_speed = smooth_curve(speed_inside_dwcl, window_size=smoothing_window)\n",
    "\n",
    "# Compute rolling standard deviation for confidence interval\n",
    "rolling_std = compute_rolling_std(speed_inside_dwcl, window_size=smoothing_window)\n",
    "\n",
    "# Define lower and upper bounds for fill_between\n",
    "lower_bound = np.maximum(smoothed_speed - rolling_std, 0)  # Ensure no negative values\n",
    "upper_bound = smoothed_speed + rolling_std\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "# Plot smoothed speed curve\n",
    "plt.plot(smoothed_speed, label=\"Smoothed Speed Inside DWCL\", color=\"royalblue\", linewidth=2,)\n",
    "\n",
    "# Adaptive confidence interval: only show fill_between if smoothing window > 1\n",
    "if smoothing_window > 1:\n",
    "    plt.fill_between(range(len(smoothed_speed)), lower_bound, upper_bound, alpha=0.2, color=\"royalblue\", label=\"Confidence Interval\")\n",
    "\n",
    "# Labels and Titles\n",
    "plt.xlabel(\"Time Steps\", fontsize=12,fontweight=\"bold\")\n",
    "plt.ylabel(\"Speed (km/h)\", fontsize=12,fontweight=\"bold\")\n",
    "#plt.title(\"Vehicle Speed Inside DWCL Over Time\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Grid and Legend\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_comparison_bars(results, metrics, titles, ylabels):\n",
    "    \"\"\"Creates horizontally arranged subplots comparing RL vs Rule-Based performance across scenarios.\"\"\"\n",
    "    \n",
    "    sns.set_palette(\"Set2\")  # Using Set2 for more distinguishable colors\n",
    "    colors_rl = [\"#1f77b4\", \"#aec7e8\"]  # RL: Blue shades\n",
    "    colors_rule = [\"#ff7f0e\", \"#ffbb78\"]  # Rule: Orange shades\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(18, 5))\n",
    "\n",
    "    soc_values = list(results[\"RL\"].keys())\n",
    "\n",
    "    for i, (metric, title, ylabel) in enumerate(zip(metrics, titles, ylabels)):\n",
    "        if metric == \"travel_time\":\n",
    "            # Replace travel time with stacked bar\n",
    "            rl_total_time = [np.mean(results[\"RL\"][soc][\"travel_time\"]) for soc in soc_values]\n",
    "            rl_inside_dwcl = [np.mean(results[\"RL\"][soc][\"time_inside_dwcl\"]) for soc in soc_values]\n",
    "            rule_total_time = [np.mean(results[\"Rule\"][soc][\"travel_time\"]) for soc in soc_values]\n",
    "            rule_inside_dwcl = [np.mean(results[\"Rule\"][soc][\"time_inside_dwcl\"]) for soc in soc_values]\n",
    "\n",
    "            x = np.arange(len(soc_values))\n",
    "            width = 0.35\n",
    "\n",
    "            # Stacked bars for RL and Rule-Based models\n",
    "            axes[i].bar(x - width/2, rl_total_time, width, label=\"RL Total Travel Time\", color=colors_rl[0], alpha=0.9)\n",
    "            axes[i].bar(x - width/2, rl_inside_dwcl, width, label=\"RL Inside DWCL\", color=colors_rl[1])\n",
    "\n",
    "            axes[i].bar(x + width/2, rule_total_time, width, label=\"Rule Total Travel Time\", color=colors_rule[0], alpha=0.9)\n",
    "            axes[i].bar(x + width/2, rule_inside_dwcl, width, label=\"Rule Inside DWCL\", color=colors_rule[1])\n",
    "\n",
    "            axes[i].set_ylabel(\"Simulation Time (s)\", fontsize=12, fontweight='bold')\n",
    "            axes[i].set_xticks(x)\n",
    "            axes[i].set_xticklabels([f\"SoC={s}\" for s in soc_values])\n",
    "            axes[i].set_title(\"Total Travel Time & DWCL Time\", fontsize=12, fontweight='bold')\n",
    "            axes[i].legend()\n",
    "\n",
    "        else:\n",
    "            # Standard bar plots for Final SOC and Lane Switches\n",
    "            rl_means = [np.mean(results[\"RL\"][soc][metric]) for soc in soc_values]\n",
    "            rule_means = [np.mean(results[\"Rule\"][soc][metric]) for soc in soc_values]\n",
    "\n",
    "            x = np.arange(len(soc_values))\n",
    "            width = 0.35\n",
    "\n",
    "            axes[i].bar(x - width/2, rl_means, width, label=\"RL-Based\", color=colors_rl[0])\n",
    "            axes[i].bar(x + width/2, rule_means, width, label=\"Rule-Based\", color=colors_rule[0])\n",
    "\n",
    "            axes[i].set_ylabel(ylabel, fontsize=12, fontweight='bold')\n",
    "            axes[i].set_xticks(x)\n",
    "            axes[i].set_xticklabels([f\"SoC={s}\" for s in soc_values])\n",
    "            axes[i].set_title(title, fontsize=12, fontweight='bold')\n",
    "            axes[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# **Plot results for all three metrics**\n",
    "metrics = [\"final_soc\", \"travel_time\", \"lane_switches\"]\n",
    "titles = [\"Final SOC Across Scenarios\", \"Total Travel Time & DWCL Time\", \"Lane Switches Across Scenarios\"]\n",
    "ylabels = [\"Final SOC (%)\", \"Time (s)\", \"Number of Switches\"]\n",
    "\n",
    "plot_comparison_bars(results, metrics, titles, ylabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_comparison_bars(results, metrics, titles, ylabels):\n",
    "    \"\"\"Creates compact subplots comparing RL vs Rule-Based performance while ensuring readability.\"\"\"\n",
    "    \n",
    "    sns.set_palette(\"Set2\")  # Use readable color scheme\n",
    "    colors_rl = [\"#1f77b4\", \"#aec7e8\"]  # RL: Blue shades\n",
    "    colors_rule = [\"#ff7f0e\", \"#ffbb78\"]  # Rule: Orange shades\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(3.2, 2.2), dpi=300)  # IEEE one-column optimized size\n",
    "\n",
    "    scenario_labels = [\"Scenario 1\", \"Scenario 2\"]  # Regular scenario labels (no bold)\n",
    "    width = 0.15  # Balanced bar width\n",
    "    x = np.array([0, 0.35])  # Adjusted spacing for compactness without overlap\n",
    "\n",
    "    for i, (metric, title, ylabel) in enumerate(zip(metrics, titles, ylabels)):\n",
    "        if metric == \"travel_time\":\n",
    "            # Stacked bars for RL and Rule-Based models\n",
    "            rl_total_time = [np.mean(results[\"RL\"][soc][\"travel_time\"]) for soc in results[\"RL\"]]\n",
    "            rl_inside_dwcl = [np.mean(results[\"RL\"][soc][\"time_inside_dwcl\"]) for soc in results[\"RL\"]]\n",
    "            rule_total_time = [np.mean(results[\"Rule\"][soc][\"travel_time\"]) for soc in results[\"Rule\"]]\n",
    "            rule_inside_dwcl = [np.mean(results[\"Rule\"][soc][\"time_inside_dwcl\"]) for soc in results[\"Rule\"]]\n",
    "\n",
    "            axes[i].bar(x - width/2, rl_total_time, width, label=\"RL Total Time\", color=colors_rl[0], alpha=0.9)\n",
    "            axes[i].bar(x - width/2, rl_inside_dwcl, width, label=\"RL Inside DWCL\", color=colors_rl[1])\n",
    "\n",
    "            axes[i].bar(x + width/2, rule_total_time, width, label=\"Rule Total Time\", color=colors_rule[0], alpha=0.9)\n",
    "            axes[i].bar(x + width/2, rule_inside_dwcl, width, label=\"Rule Inside DWCL\", color=colors_rule[1])\n",
    "\n",
    "            axes[i].set_ylabel(\"Time (s)\", fontsize=4, fontweight='bold')\n",
    "            axes[i].set_xticks(x)\n",
    "            axes[i].set_xticklabels(scenario_labels, fontsize=4)  # Reduced font size to 3, removed bold\n",
    "            axes[i].set_title(\"Travel Time & DWCL Time\", fontsize=4, fontweight='bold')\n",
    "            axes[i].legend(fontsize=3.4, frameon=True)\n",
    "\n",
    "        else:\n",
    "            # Standard bar plots for Final SOC and Lane Switches\n",
    "            rl_means = [np.mean(results[\"RL\"][soc][metric]) for soc in results[\"RL\"]]\n",
    "            rule_means = [np.mean(results[\"Rule\"][soc][metric]) for soc in results[\"Rule\"]]\n",
    "\n",
    "            axes[i].bar(x - width/2, rl_means, width, label=\"RL-Based\", color=colors_rl[0])\n",
    "            axes[i].bar(x + width/2, rule_means, width, label=\"Rule-Based\", color=colors_rule[0])\n",
    "\n",
    "            axes[i].set_ylabel(ylabel, fontsize=4, fontweight='bold')\n",
    "            axes[i].set_xticks(x)\n",
    "            axes[i].set_xticklabels(scenario_labels, fontsize=4)  # Reduced font size to 3, removed bold\n",
    "            axes[i].set_title(title, fontsize=4, fontweight='bold')\n",
    "            axes[i].legend(fontsize=3.4, loc=\"upper right\", frameon=True)\n",
    "\n",
    "        # **Ensure y-axis tick labels have fontsize 4**\n",
    "        axes[i].tick_params(axis='y', labelsize=4)\n",
    "        \n",
    "        # **Increase Y-axis spacing**\n",
    "        axes[i].margins(y=0.4)  # Adjusted for better spacing\n",
    "\n",
    "    # **Increased spacing among subplots**\n",
    "    plt.subplots_adjust(left=0.2, right=0.95, top=0.85, bottom=0.3, wspace=0.5)  # Increased `wspace` for more separation\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# **Plot results for all three metrics**\n",
    "metrics = [\"final_soc\", \"travel_time\", \"lane_switches\"]\n",
    "titles = [\"Final SOC\", \"Travel Time & DWCL Time\", \"Lane Switches\"]\n",
    "ylabels = [\"Final SOC (%)\", \"Time (s)\", \"No. of Switches\"]\n",
    "\n",
    "plot_comparison_bars(results, metrics, titles, ylabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries since the execution state was reset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# Compute the average for each metric\n",
    "metrics = [\"Final SOC (%)\", \"Total Travel Time (s)\", \"Total Lane Switches\"]\n",
    "rl_means = [np.mean(rl_soc), np.mean(rl_time), np.mean(rl_switches)]\n",
    "rule_means = [np.mean(rule_soc), np.mean(rule_time), np.mean(rule_switches)]\n",
    "\n",
    "# Define bar width and positions\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "# Create bar plot\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars1 = ax.bar(x - width/2, rl_means, width, label=\"RL-Based\", color=\"blue\", alpha=0.7)\n",
    "bars2 = ax.bar(x + width/2, rule_means, width, label=\"Rule-Based\", color=\"orange\", alpha=0.7)\n",
    "\n",
    "# Labels and Title\n",
    "ax.set_xlabel(\"Metrics\")\n",
    "ax.set_ylabel(\"Average Value\")\n",
    "ax.set_title(\"Comparison of RL-Based and Rule-Based Performance\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "# Show values on top of bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f\"{height:.1f}\",\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 5),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_metrics()#add simulation time for each episode and if it arrives or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "threshold = 1400\n",
    "# Set improved color palette\n",
    "sns.set_palette(\"colorblind\")\n",
    "reward_color = sns.color_palette(\"coolwarm\", as_cmap=True)(0.2)  # Blueish\n",
    "loss_color = sns.color_palette(\"coolwarm\", as_cmap=True)(0.8)  # Reddish\n",
    "\n",
    "# Ensure numerical stability by removing NaN values from losses\n",
    "filtered_losses = np.array(losses[threshold:])\n",
    "filtered_losses = filtered_losses[~np.isnan(filtered_losses)]  # Remove NaNs\n",
    "\n",
    "# Smoothed Rewards and Confidence Interval\n",
    "smoothed_rewards = smooth_curve(episode_rewards[threshold:], window_size=50)\n",
    "std_rewards = np.std(episode_rewards[threshold:])  # Compute standard deviation\n",
    "\n",
    "# Smoothed Losses and Confidence Interval (avoiding NaN issues)\n",
    "smoothed_losses = smooth_curve(filtered_losses, window_size=50)\n",
    "std_losses = np.std(filtered_losses) if len(filtered_losses) > 0 else 0  # Ensure stability\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Adjust x-axis to start from episode threshold but display as episode 0\n",
    "x_range = np.arange(len(smoothed_rewards))\n",
    "\n",
    "# Plot Reward Curve\n",
    "ax1.plot(x_range, smoothed_rewards, label=\" Rewards\", color=reward_color, linewidth=2)\n",
    "ax1.fill_between(x_range, \n",
    "                 smoothed_rewards - std_rewards, \n",
    "                 smoothed_rewards + std_rewards, \n",
    "                 alpha=0.2, color=reward_color)\n",
    "\n",
    "ax1.set_xlabel(\"Episode\", fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel(\"Reward\", color=reward_color, fontsize=12, fontweight='bold')\n",
    "ax1.tick_params(axis='y', labelcolor=reward_color)\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Create second y-axis for Loss\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x_range[:len(smoothed_losses)], smoothed_losses, label=\" Loss\", color=loss_color, linestyle=\"dashed\", linewidth=2)\n",
    "\n",
    "# Ensure confidence interval does not go below zero\n",
    "lower_loss_bound = np.maximum(smoothed_losses - std_losses, 0)\n",
    "ax2.fill_between(x_range[:len(smoothed_losses)], \n",
    "                 lower_loss_bound, \n",
    "                 smoothed_losses + std_losses, \n",
    "                 alpha=0.2, color=loss_color)\n",
    "\n",
    "ax2.set_ylabel(\"Loss (MSE)\", color=loss_color, fontsize=12, fontweight='bold')\n",
    "ax2.tick_params(axis='y', labelcolor=loss_color)\n",
    "\n",
    "# Add legends\n",
    "fig.legend(loc=\"upper left\", bbox_to_anchor=(0.1, 0.97), fontsize=12)\n",
    "ax1.set_xlim(0,870)\n",
    "# Titles and Layout Improvements\n",
    "#plt.title(\"Episode Rewards and Loss per Episode with Confidence Intervals\", fontsize=14, fontweight='bold', pad=15)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "#start_index=2500\n",
    "def plot_soc_and_time_enhanced(soc_values, times, episodes, start_index=1, display_start=1100):\n",
    "    \"\"\"\n",
    "    Generates an enhanced heatmap visualization for SoC and Travel Time over episodes.\n",
    "    - Uses perceptually uniform colormaps.\n",
    "    - Applies better contrast scaling.\n",
    "    - Avoids x-axis overcrowding for clarity.\n",
    "\n",
    "    Parameters:\n",
    "    - soc_values (list): Battery State of Charge (SoC) per episode.\n",
    "    - times (list): Travel time per episode.\n",
    "    - episodes (list): Episode indices.\n",
    "    - start_index (int): Index to start plotting from.\n",
    "    - display_start (int): Displayed x-axis start value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Trim data based on start index\n",
    "    soc_values = np.array(soc_values[start_index:])\n",
    "    times = np.array(times[start_index:])\n",
    "    episodes = np.arange(display_start, display_start + len(soc_values))  # Shift episode numbers\n",
    "\n",
    "    # Downsample for better visualization (keep around 500 points)\n",
    "    sampling_rate = max(1, len(episodes) // 500)\n",
    "    sampled_episodes = episodes[::sampling_rate]\n",
    "    sampled_soc = soc_values[::sampling_rate]\n",
    "    sampled_times = times[::sampling_rate]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_sampled = pd.DataFrame({\n",
    "        \"Episodes\": sampled_episodes,\n",
    "        \"SoC\": sampled_soc,\n",
    "        \"Time\": sampled_times\n",
    "    })\n",
    "\n",
    "    # Create figure with shared x-axis\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12, 6), sharex=True, gridspec_kw={'hspace': 0.05})\n",
    "\n",
    "    # Adjust x-ticks to prevent overcrowding\n",
    "    num_xticks = 10  # Show only 10 evenly spaced ticks\n",
    "    xtick_positions = np.linspace(0, len(sampled_episodes) - 1, num_xticks, dtype=int)\n",
    "    xtick_labels = [sampled_episodes[i] for i in xtick_positions]\n",
    "\n",
    "    # Normalize color scale for better contrast\n",
    "    norm_soc = Normalize(vmin=max(0, np.min(sampled_soc)), vmax=np.max(sampled_soc))\n",
    "    norm_time = Normalize(vmin=max(0, np.min(sampled_times)), vmax=np.max(sampled_times))\n",
    "\n",
    "    # First heatmap: SoC (Using \"coolwarm\" for balanced color differentiation)\n",
    "    sns.heatmap(np.expand_dims(df_sampled[\"SoC\"], axis=0), cmap=\"inferno\", cbar=True, ax=ax[0],\n",
    "                norm=norm_soc, xticklabels=False, yticklabels=[\"SoC (%)\"])\n",
    "    #ax[0].set_title(\"Battery SoC and Travel Time Across Episodes\")\n",
    "\n",
    "    # Second heatmap: Travel Time (Using \"cividis\" for perceptual clarity)\n",
    "    sns.heatmap(np.expand_dims(df_sampled[\"Time\"], axis=0), cmap=\"twilight\", cbar=True, ax=ax[1],\n",
    "                norm=norm_time, xticklabels=xtick_labels, yticklabels=[\"Travel Time (s)\"])\n",
    "    ax[1].set_xlabel(\"Episodes\")\n",
    "\n",
    "    # Adjust x-ticks for clarity\n",
    "    ax[1].set_xticks(xtick_positions)\n",
    "    ax[1].set_xticklabels(xtick_labels, rotation=45)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with simulated data\n",
    "episodes = np.arange(len(soc_values))  # Simulating episode numbers\n",
    "\n",
    "\n",
    "# Call function with specified start index and display shift\n",
    "plot_soc_and_time_enhanced(soc_values, times, episodes, start_index=0, display_start=1100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries after execution state reset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Reinitialize sample data since previous variables were lost\n",
    "# Assuming `invalid_action_counts` and `switch_frequencies` were preloaded lists\n",
    "episodes = np.arange(len(invalid_action_counts))  # Simulating episode numbers\n",
    "\n",
    "# Define a rolling average function ensuring values remain positive\n",
    "def rolling_average_positive(data, window_size=50):\n",
    "    \"\"\"Computes rolling average and ensures values remain non-negative.\"\"\"\n",
    "    avg = np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "    return np.maximum(avg, 0)  # Ensure values stay non-negative\n",
    "\n",
    "# Start from episode 1400 (adjust x-axis accordingly)\n",
    "start_idx = 1400\n",
    "window_size = 50\n",
    "adjusted_episodes = np.arange(0, len(episodes[start_idx:]) - (window_size - 1))  # Show as episode 0 onwards\n",
    "\n",
    "# Compute rolling averages\n",
    "rolling_invalid_actions = rolling_average_positive(invalid_action_counts[start_idx:], window_size)\n",
    "rolling_lane_switches = rolling_average_positive(switch_frequencies[start_idx:], window_size)\n",
    "\n",
    "# Compute standard deviation ensuring values remain above zero\n",
    "std_invalid = np.std(invalid_action_counts[start_idx:])\n",
    "std_switches = np.std(switch_frequencies[start_idx:])\n",
    "\n",
    "# Ensure confidence intervals don't go below zero\n",
    "lower_bound_invalid = np.maximum(rolling_invalid_actions - std_invalid, 0)\n",
    "upper_bound_invalid = rolling_invalid_actions + std_invalid\n",
    "\n",
    "lower_bound_switches = np.maximum(rolling_lane_switches - std_switches, 0)\n",
    "upper_bound_switches = rolling_lane_switches + std_switches\n",
    "\n",
    "# Set colorblind-friendly palette\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "# Define colors using 'coolwarm' colormap\n",
    "coolwarm_cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "invalid_action_color = coolwarm_cmap(0.2)  # Blueish tone\n",
    "lane_switch_color = coolwarm_cmap(0.8)  # Reddish tone\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Rolling Average Line Plot with Confidence Interval\n",
    "ax.plot(adjusted_episodes, rolling_invalid_actions, label=\"Invalid Actions\", color=invalid_action_color, linewidth=2)\n",
    "ax.fill_between(adjusted_episodes, lower_bound_invalid, upper_bound_invalid, alpha=0.2, color=invalid_action_color)\n",
    "\n",
    "ax.plot(adjusted_episodes, rolling_lane_switches, label=\"Lane Switches\", color=lane_switch_color, linewidth=2, linestyle=\"dashed\")\n",
    "ax.fill_between(adjusted_episodes, lower_bound_switches, upper_bound_switches, alpha=0.2, color=lane_switch_color)\n",
    "\n",
    "# Labels and Title\n",
    "ax.set_xlabel(\"Episode\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Count\", fontsize=12, fontweight='bold')\n",
    "#ax.set_title(\"Rolling Average of Invalid Actions & Lane Switches Over Time\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#episode 4900 i added 20 to this --->  if in_dwcl and SoC < required_soc + 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a rule based agent and compare it with the DQN agent; over some test episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when invalide action occu , penalize the agent by editing the q value of the action taken to be negative, so the agent will avoid taking it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add total travel time to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gamma 0.95 --> 0.9 at episod 2240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a scalled reward function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use elapsed time/ eta in state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use  abetter NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix the exit_dwcl function when there is no left lane, the vehicle should stay in the current lane,\n",
    "#also add a condition to check if the vehicle is in the destination lane, if so, the vehicle should \n",
    "#stop and the simulation should end. otherwise a timeout should be triggered and the simulation should end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_point.location.x + length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.destination.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.vehicle.get_location().x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.battery_manager.soc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sim_env()\n",
    "# Load power model\n",
    "power_model = load_model(\"c:\\\\Users\\\\HOUALEF\\\\Desktop\\\\RL_Carla\\\\models\\\\energy_model_best.hdf5\")\n",
    "\n",
    "\n",
    "spawn_points = world.get_map().get_spawn_points()\n",
    "destination = carla.Location(\n",
    "    x=start_point.location.x + length,\n",
    "    y=start_point.location.y,\n",
    "    z=start_point.location.z,\n",
    ")\n",
    "\n",
    "# Initialize battery manager\n",
    "battery_manager = BatteryManager(vehicle, destination, world, power_model,C_bat,P_aux)\n",
    "env = CarlaEnv(vehicle, battery_manager, all_corners, destination)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vehicle.set_autopilot(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.stop_car()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle.set_autopilot(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vehicle.set_autopilot(True)\n",
    "control = carla.VehicleControl()\n",
    "control.throttle = 0.0\n",
    "control.brake = 1.0  # Full brake\n",
    "control.steer = +0.00\n",
    "# âœ… Apply stopping control\n",
    "vehicle.apply_control(control)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.battery_manager.is_on_charging_lane()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.vehicle.set_transform(start_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    env.move_to_dwcl()\n",
    "    time.sleep(5)\n",
    "    env.exit_dwcl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.move_to_dwcl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.tm.vehicle_percentage_speed_difference(vehicle, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_speed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.speed_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.slow_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.exit_dwcl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vehicle.get_speed_limit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    velocity = vehicle.get_velocity()\n",
    "    speed_m_s = math.sqrt(velocity.x**2 + velocity.y**2) *3.6\n",
    "    print('Speed:',speed_m_s,'   Max_speed: ' ,vehicle.get_speed_limit(),'\\n')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nnwww\n",
    "import random\n",
    "import collections\n",
    "from gym import spaces\n",
    "\n",
    "import carla\n",
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class CarlaEnv(gym.Env):\n",
    "    def __init__(self, vehicle, battery_manager, all_corners, destination):\n",
    "        super(CarlaEnv, self).__init__()\n",
    "        self.vehicle = vehicle\n",
    "        self.battery_manager = battery_manager\n",
    "        self.all_corners = all_corners\n",
    "        self.destination = destination\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        # Define state space: [SoC, Required SoC, ETA, Distance to DWCL End, Lane Type]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0, 0]),\n",
    "            high=np.array([100, 100, 1000, 500, 1]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Define action space: [Go to DWCL, Leave DWCL, Accelerate, Decelerate, Maintain Speed]\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "    def find_nearest_dwcl(self):\n",
    "        \n",
    "        vehicle_location = self.vehicle.get_location()\n",
    "        min_distance = float('inf')\n",
    "        nearest_dwcl = None\n",
    "\n",
    "        for corners in self.all_corners:\n",
    "            center_x = sum(c.x for c in corners) / 4\n",
    "            center_y = sum(c.y for c in corners) / 4\n",
    "            dwcl_center = carla.Location(center_x, center_y, corners[0].z)\n",
    "\n",
    "            distance = vehicle_location.distance(dwcl_center)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                nearest_dwcl = dwcl_center\n",
    "\n",
    "        return nearest_dwcl\n",
    "\n",
    "    def change_lane(self, target_lane):\n",
    "        \n",
    "        self.vehicle.set_autopilot(False)\n",
    "        agent = BehaviorAgent(self.vehicle, behavior=\"normal\")\n",
    "        agent.set_destination(target_lane)\n",
    "        self.vehicle.set_autopilot(True)\n",
    "\n",
    "    def move_to_nearest_dwcl(self):\n",
    "        \n",
    "        nearest_dwcl = self.find_nearest_dwcl()\n",
    "        if nearest_dwcl:\n",
    "            self.change_lane(nearest_dwcl)\n",
    "        else:\n",
    "            print(\"No DWCL found nearby!\")\n",
    "\n",
    "    def leave_dwcl(self):\n",
    "        \n",
    "        vehicle_location = self.vehicle.get_location()\n",
    "        map = self.vehicle.get_world().get_map()\n",
    "        waypoint = map.get_waypoint(vehicle_location, project_to_road=True, lane_type=carla.LaneType.Driving)\n",
    "        \n",
    "        while waypoint.lane_type == carla.LaneType.Driving and waypoint.lane_id % 2 == 0:\n",
    "            waypoint = waypoint.get_left_lane()\n",
    "            if waypoint is None:\n",
    "                print(\"No non-DWCL lane found!\")\n",
    "                return\n",
    "        \n",
    "        self.change_lane(waypoint.transform.location)\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        eta = self.battery_manager.eta\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        distance_to_destination = self.battery_manager.compute_remaining_distance()\n",
    "        lane_type = int(self.battery_manager.is_on_charging_lane(self.all_corners)[0])\n",
    "\n",
    "        # End episode if destination is reached or time exceeds 3x ETA\n",
    "        if distance_to_destination < 1 or elapsed_time > 3 * eta:\n",
    "            return np.array([0, 0, 0, 0, lane_type]), 0, True, {}\n",
    "        \n",
    "        reward = 0\n",
    "        if action == 0:  # Move to nearest DWCL\n",
    "            if lane_type == 0:\n",
    "                self.move_to_nearest_dwcl()\n",
    "                reward += 10  # Reward entering DWCL when necessary\n",
    "            else:\n",
    "                reward -= 5  # Penalize unnecessary lane changes\n",
    "        elif action == 1:  # Leave DWCL\n",
    "            if lane_type == 1:\n",
    "                self.leave_dwcl()\n",
    "                reward += 5  # Reward leaving DWCL when unnecessary\n",
    "            else:\n",
    "                reward -= 5\n",
    "        elif action == 2:  # Accelerate inside DWCL\n",
    "            if lane_type == 1:\n",
    "                self.vehicle.set_autopilot(False)\n",
    "                self.vehicle.apply_control(carla.VehicleControl(throttle=0.8))\n",
    "                self.vehicle.set_autopilot(True)\n",
    "                reward += 3  # Reward acceleration when charging is optimal\n",
    "        elif action == 3:  # Decelerate inside DWCL\n",
    "            if lane_type == 1:\n",
    "                self.vehicle.set_autopilot(False)\n",
    "                self.vehicle.apply_control(carla.VehicleControl(throttle=0.3, brake=0.2))\n",
    "                self.vehicle.set_autopilot(True)\n",
    "                reward += 3  # Reward slowing down when necessary\n",
    "        elif action == 4:  # Maintain speed inside DWCL\n",
    "            if lane_type == 1:\n",
    "                self.vehicle.set_autopilot(False)\n",
    "                self.vehicle.apply_control(carla.VehicleControl(throttle=0.5))\n",
    "                self.vehicle.set_autopilot(True)\n",
    "                reward += 2  # Reward maintaining speed for efficient charging\n",
    "\n",
    "        # Compute updated state\n",
    "        state = np.array([self.battery_manager.soc, self.battery_manager.soc_required, eta, distance_to_destination, lane_type])\n",
    "        return state, reward, False, {}\n",
    "\n",
    "    def reset(self):\n",
    "       \n",
    "        create_sim_env()\n",
    "        \n",
    "       \n",
    "        self.vehicle.set_autopilot(True)\n",
    "        \n",
    "        self.soc, soc_required, self.total_energy, ETA, P_ev, f_align, k, eta_transfer, remaining_distance, acceleration, slope = self.battery_manager.update_battery(0,self.all_corners)\n",
    "        state = np.array([self.soc, soc_required, ETA, remaining_distance, int(self.battery_manager.is_on_charging_lane(self.all_corners)[0])])\n",
    "        return state\n",
    "\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.memory = collections.deque(maxlen=10000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.05\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, 4)  # Random action\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.model(state)).item()\n",
    "\n",
    "    def train(self, batch_size=64):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.model(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = nn.functional.mse_loss(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Constants\n",
    "eta_ch = 0.92  # Battery charging efficiency\n",
    "P_ev_max = 80  # Maximum Power Transfer (40 kW)\n",
    "Q1 = 90  # Quality factor of primary coil\n",
    "Q2 = 90 # Quality factor of secondary coil\n",
    "k0 = 0.2  # Nominal coupling coefficient (perfect alignment)\n",
    "d_max = 0.5  # Maximum lateral misalignment before no power transfer (meters)\n",
    "\n",
    "# Vehicle constants\n",
    "m = 1680  # kg, Mass\n",
    "g = 9.81  # m/s^2, Gravity\n",
    "c_rr = 0.01  # Rolling resistance coefficient\n",
    "c_d = 0.28  # Drag coefficient\n",
    "A = 1.93  # m^2, Frontal area\n",
    "rho = 1.20  # kg/m^3, Air density\n",
    "C_bat = 24  # kWh, Battery capacity\n",
    "alpha = 0.90  # Transmission efficiency\n",
    "beta = 0.75  # Regeneration efficiency\n",
    "P_aux = 0  # kW, Auxiliary power consumption\n",
    "max_speed = 70 / 3.6  # m/s\n",
    "avg_speed = 35 / 3.6\n",
    "avg_slop = 0.002\n",
    "avg_acc = 0.28\n",
    "\n",
    "create_sim_env()\n",
    "# Load power model\n",
    "power_model = load_model(\"c:\\\\Users\\\\HOUALEF\\\\Desktop\\\\RL_Carla\\\\models\\\\energy_model_best.hdf5\")\n",
    "\n",
    "spawn_points = world.get_map().get_spawn_points()\n",
    "destination = carla.Location(\n",
    "    x=start_point.location.x - length,\n",
    "    y=start_point.location.y,\n",
    "    z=start_point.location.z,\n",
    ")\n",
    "\n",
    "# Initialize battery manager\n",
    "battery_manager = BatteryManager(vehicle, destination, world, power_model,C_bat,P_aux)\n",
    "\n",
    "\n",
    "# Initialize Environment and Agent\n",
    "env = CarlaEnv(vehicle, battery_manager, all_corners,destination)\n",
    "agent = DQNAgent(state_dim=5, action_dim=5)\n",
    "\n",
    "num_episodes = 500  # Number of episodes to train\n",
    "max_steps = 1000  # Max steps per episode\n",
    "batch_size = 64  # Experience replay batch size\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Select action using Îµ-greedy policy\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        # Execute action in environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store experience in memory\n",
    "        agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Train the agent\n",
    "        agent.train(batch_size)\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # End episode if done\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Print training progress\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(agent.model.state_dict(), \"dqn_carla.pth\")\n",
    "print(\"Training complete! Model saved.\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation ended.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Constants\n",
    "eta_ch = 0.92  # Battery charging efficiency\n",
    "P_ev_max = 80  # Maximum Power Transfer (40 kW)\n",
    "Q1 = 90  # Quality factor of primary coil\n",
    "Q2 = 90 # Quality factor of secondary coil\n",
    "k0 = 0.2  # Nominal coupling coefficient (perfect alignment)\n",
    "d_max = 0.5  # Maximum lateral misalignment before no power transfer (meters)\n",
    "\n",
    "# Vehicle constants\n",
    "m = 1680  # kg, Mass\n",
    "g = 9.81  # m/s^2, Gravity\n",
    "c_rr = 0.01  # Rolling resistance coefficient\n",
    "c_d = 0.28  # Drag coefficient\n",
    "A = 1.93  # m^2, Frontal area\n",
    "rho = 1.20  # kg/m^3, Air density\n",
    "C_bat = 24  # kWh, Battery capacity\n",
    "alpha = 0.90  # Transmission efficiency\n",
    "beta = 0.75  # Regeneration efficiency\n",
    "P_aux = 0  # kW, Auxiliary power consumption\n",
    "max_speed = 70 / 3.6  # m/s\n",
    "avg_speed = 35 / 3.6\n",
    "avg_slop = 0.002\n",
    "avg_acc = 0.28\n",
    "\n",
    "# Pygame initialization\n",
    "pygame.init()\n",
    "WIDTH, HEIGHT = 1600, 900  # Window size\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"CARLA Vehicle Info Display\")\n",
    "font = pygame.font.Font(None, 24)\n",
    "clock = pygame.time.Clock()\n",
    "create_sim_env()\n",
    "# Connect to CARLA\n",
    "settings = world.get_settings()\n",
    "settings.synchronous_mode = True\n",
    "world.apply_settings(settings)\n",
    "\n",
    "spawn_points = world.get_map().get_spawn_points()\n",
    "destination = carla.Location(\n",
    "    x=start_point.location.x - length,\n",
    "    y=start_point.location.y,\n",
    "    z=start_point.location.z,\n",
    ")\n",
    "\n",
    "# Load power model\n",
    "power_model = load_model(\"c:\\\\Users\\\\HOUALEF\\\\Desktop\\\\RL_Carla\\\\models\\\\energy_model_best.hdf5\")\n",
    "\n",
    "# Initialize battery manager\n",
    "battery_manager = BatteryManager(vehicle, destination, world, power_model,C_bat,P_aux)\n",
    "\n",
    "# Attach a camera to the vehicle\n",
    "camera_bp = world.get_blueprint_library().find(\"sensor.camera.rgb\")\n",
    "camera_bp.set_attribute(\"image_size_x\", str(WIDTH))\n",
    "camera_bp.set_attribute(\"image_size_y\", str(HEIGHT))\n",
    "camera_bp.set_attribute(\"fov\", \"110\")\n",
    "camera_transform = carla.Transform(carla.Location(x=1.5, z=2.0))\n",
    "camera = world.spawn_actor(camera_bp, camera_transform, attach_to=vehicle)\n",
    "latest_image = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "def process_img(image):\n",
    "    global latest_image\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "    array = np.reshape(array, (image.height, image.width, 4))\n",
    "    latest_image = cv2.flip(array[:, :, :3], 1)\n",
    "\n",
    "camera.listen(lambda image: process_img(image))\n",
    "\n",
    "# Agent for automatic driving\n",
    "agent = BehaviorAgent(vehicle, behavior=\"normal\")\n",
    "agent.set_destination(destination)\n",
    "\n",
    "manual_mode = True\n",
    "control = carla.VehicleControl()\n",
    "\n",
    "# Main loop\n",
    "running = True\n",
    "while running:\n",
    "    world.tick()\n",
    "    #clock.tick(20)\n",
    "    \n",
    "    current_time = battery_manager.get_simulation_time()\n",
    "    delta_time = current_time - battery_manager.last_update_time\n",
    "    battery_manager.last_update_time = current_time\n",
    "    \n",
    "    soc, soc_required, total_energy, ETA, charging_power, f_align, k, eta_transfer, remaining_distance,acceleration,slope = battery_manager.update_battery(\n",
    "        delta_time, all_corners\n",
    "    )\n",
    "    \n",
    "    # Manual control\n",
    "    keys = pygame.key.get_pressed()\n",
    "    if keys[pygame.K_m]:\n",
    "        manual_mode = not manual_mode\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    if manual_mode:\n",
    "        control.throttle = 1.0 if keys[pygame.K_UP] else 0.0\n",
    "        control.brake = 1.0 if keys[pygame.K_DOWN] else 0.0\n",
    "        control.steer = -0.5 if keys[pygame.K_LEFT] else (0.5 if keys[pygame.K_RIGHT] else 0.0)\n",
    "        control.reverse = keys[pygame.K_SPACE]\n",
    "    else:\n",
    "        control = agent.run_step()\n",
    "    \n",
    "    vehicle.apply_control(control)\n",
    "    \n",
    "    # Pygame Display Update\n",
    "    screen.fill((0, 0, 0))\n",
    "    frame = cv2.cvtColor(latest_image, cv2.COLOR_BGR2RGB)\n",
    "    frame = np.rot90(frame)\n",
    "    frame = pygame.surfarray.make_surface(frame)\n",
    "    screen.blit(frame, (0, 0))\n",
    "    map = vehicle.get_world().get_map()\n",
    "    waypoint = map.get_waypoint(vehicle.get_location())\n",
    "    lane_id = waypoint.lane_id\n",
    "    \n",
    "    text_data = [\n",
    "        f\"Driving Mode: {'Manual' if manual_mode else 'Automatic'}\",\n",
    "        f\"Speed: {vehicle.get_velocity().length() * 3.6:.2f} km/h\",\n",
    "        f\"SoC: {soc:.2f}%\",\n",
    "        f\"Energy Consumed: {total_energy:.4f} kWh\",\n",
    "        f\"ETA: {ETA:.2f}s\",\n",
    "        f\"Required SoC: {soc_required:.2f}%\",\n",
    "        f\"Remaining Distance: {remaining_distance:.2f}m\",\n",
    "        f\"Charging Power: {charging_power:.2f} kW\",\n",
    "        f\"Coupling k: {k:.2f}\",\n",
    "        f\"Alignment: {f_align:.2f}\",\n",
    "        f\"Eta Transfer: {eta_transfer:.2f}\",\n",
    "        f\"Delta Time: {delta_time:.2f} s\" ,\n",
    "        f\"Acceleration: {acceleration:.2f} m/sÂ²\",\n",
    "        f\"Slope: {slope:.2f} Â°\",\n",
    "        f\"lane_id: {lane_id}\"\n",
    "    ]\n",
    "    \n",
    "    y_offset = 30\n",
    "    for text in text_data:\n",
    "        screen.blit(font.render(text, True, (255, 255, 255)), (20, y_offset))\n",
    "        y_offset += 30\n",
    "    \n",
    "    pygame.display.flip()\n",
    "    \n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT or soc <= 0:\n",
    "            running = False\n",
    "\n",
    "pygame.quit()\n",
    "camera.destroy()\n",
    "print(\"Simulation ended.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
